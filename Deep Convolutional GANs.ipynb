{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hvyd/anaconda3/lib/python3.5/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batchSize = 64 \n",
    "imageSize = 64 \n",
    "\n",
    "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) \n",
    "\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform) \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = G()\n",
    "netG.apply(weights_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)\n",
    "\n",
    "netD = D()\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the DCGANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hvyd/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:38: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_D: 1.8682 Loss_G: 4.5517\n",
      "[0/25][1/782] Loss_D: 1.3389 Loss_G: 6.0139\n",
      "[0/25][2/782] Loss_D: 0.8093 Loss_G: 5.5869\n",
      "[0/25][3/782] Loss_D: 0.9610 Loss_G: 6.1590\n",
      "[0/25][4/782] Loss_D: 0.7666 Loss_G: 6.6839\n",
      "[0/25][5/782] Loss_D: 1.0948 Loss_G: 5.8599\n",
      "[0/25][6/782] Loss_D: 1.2627 Loss_G: 8.2935\n",
      "[0/25][7/782] Loss_D: 0.6870 Loss_G: 7.9181\n",
      "[0/25][8/782] Loss_D: 0.6718 Loss_G: 7.8171\n",
      "[0/25][9/782] Loss_D: 0.6594 Loss_G: 9.2848\n",
      "[0/25][10/782] Loss_D: 0.8783 Loss_G: 8.6805\n",
      "[0/25][11/782] Loss_D: 0.9751 Loss_G: 10.2034\n",
      "[0/25][12/782] Loss_D: 0.6777 Loss_G: 7.8494\n",
      "[0/25][13/782] Loss_D: 1.1714 Loss_G: 12.6030\n",
      "[0/25][14/782] Loss_D: 0.3321 Loss_G: 9.7251\n",
      "[0/25][15/782] Loss_D: 0.7122 Loss_G: 10.3030\n",
      "[0/25][16/782] Loss_D: 0.5205 Loss_G: 10.7529\n",
      "[0/25][17/782] Loss_D: 0.3464 Loss_G: 9.9848\n",
      "[0/25][18/782] Loss_D: 0.6510 Loss_G: 12.6782\n",
      "[0/25][19/782] Loss_D: 0.2297 Loss_G: 9.3830\n",
      "[0/25][20/782] Loss_D: 0.6528 Loss_G: 14.4323\n",
      "[0/25][21/782] Loss_D: 0.1505 Loss_G: 12.4194\n",
      "[0/25][22/782] Loss_D: 0.2451 Loss_G: 6.9026\n",
      "[0/25][23/782] Loss_D: 2.5903 Loss_G: 20.0474\n",
      "[0/25][24/782] Loss_D: 0.6049 Loss_G: 21.2825\n",
      "[0/25][25/782] Loss_D: 0.3946 Loss_G: 16.3511\n",
      "[0/25][26/782] Loss_D: 0.1331 Loss_G: 6.8355\n",
      "[0/25][27/782] Loss_D: 3.2867 Loss_G: 19.4629\n",
      "[0/25][28/782] Loss_D: 0.2107 Loss_G: 21.5314\n",
      "[0/25][29/782] Loss_D: 0.4759 Loss_G: 17.3986\n",
      "[0/25][30/782] Loss_D: 0.0685 Loss_G: 8.6960\n",
      "[0/25][31/782] Loss_D: 1.4117 Loss_G: 17.3992\n",
      "[0/25][32/782] Loss_D: 0.0973 Loss_G: 18.8588\n",
      "[0/25][33/782] Loss_D: 0.1570 Loss_G: 15.5408\n",
      "[0/25][34/782] Loss_D: 0.2957 Loss_G: 7.8140\n",
      "[0/25][35/782] Loss_D: 1.3110 Loss_G: 17.5109\n",
      "[0/25][36/782] Loss_D: 0.1445 Loss_G: 18.4883\n",
      "[0/25][37/782] Loss_D: 0.1363 Loss_G: 14.8532\n",
      "[0/25][38/782] Loss_D: 0.0852 Loss_G: 7.4109\n",
      "[0/25][39/782] Loss_D: 1.7322 Loss_G: 20.0977\n",
      "[0/25][40/782] Loss_D: 0.3361 Loss_G: 22.2497\n",
      "[0/25][41/782] Loss_D: 0.1373 Loss_G: 19.0231\n",
      "[0/25][42/782] Loss_D: 0.1410 Loss_G: 12.1589\n",
      "[0/25][43/782] Loss_D: 0.1391 Loss_G: 5.0439\n",
      "[0/25][44/782] Loss_D: 2.8740 Loss_G: 22.5134\n",
      "[0/25][45/782] Loss_D: 0.3584 Loss_G: 26.0117\n",
      "[0/25][46/782] Loss_D: 0.1635 Loss_G: 25.8540\n",
      "[0/25][47/782] Loss_D: 0.2022 Loss_G: 23.2395\n",
      "[0/25][48/782] Loss_D: 0.1259 Loss_G: 18.4497\n",
      "[0/25][49/782] Loss_D: 0.1006 Loss_G: 11.7998\n",
      "[0/25][50/782] Loss_D: 0.0311 Loss_G: 6.0253\n",
      "[0/25][51/782] Loss_D: 0.7990 Loss_G: 15.4807\n",
      "[0/25][52/782] Loss_D: 0.0684 Loss_G: 17.2381\n",
      "[0/25][53/782] Loss_D: 0.0685 Loss_G: 15.1406\n",
      "[0/25][54/782] Loss_D: 0.0880 Loss_G: 10.1426\n",
      "[0/25][55/782] Loss_D: 0.0871 Loss_G: 5.3480\n",
      "[0/25][56/782] Loss_D: 1.4067 Loss_G: 19.2705\n",
      "[0/25][57/782] Loss_D: 0.2306 Loss_G: 22.5841\n",
      "[0/25][58/782] Loss_D: 0.3884 Loss_G: 19.6674\n",
      "[0/25][59/782] Loss_D: 0.1257 Loss_G: 13.2555\n",
      "[0/25][60/782] Loss_D: 0.0621 Loss_G: 5.5942\n",
      "[0/25][61/782] Loss_D: 2.1671 Loss_G: 24.6202\n",
      "[0/25][62/782] Loss_D: 1.2487 Loss_G: 25.9368\n",
      "[0/25][63/782] Loss_D: 0.9398 Loss_G: 22.8282\n",
      "[0/25][64/782] Loss_D: 0.0231 Loss_G: 16.3269\n",
      "[0/25][65/782] Loss_D: 0.0872 Loss_G: 6.6092\n",
      "[0/25][66/782] Loss_D: 1.1030 Loss_G: 17.2876\n",
      "[0/25][67/782] Loss_D: 0.0595 Loss_G: 19.2851\n",
      "[0/25][68/782] Loss_D: 0.3852 Loss_G: 15.8048\n",
      "[0/25][69/782] Loss_D: 0.0825 Loss_G: 11.0598\n",
      "[0/25][70/782] Loss_D: 0.0779 Loss_G: 4.9094\n",
      "[0/25][71/782] Loss_D: 1.5201 Loss_G: 17.3683\n",
      "[0/25][72/782] Loss_D: 0.8826 Loss_G: 18.6678\n",
      "[0/25][73/782] Loss_D: 0.5055 Loss_G: 13.6816\n",
      "[0/25][74/782] Loss_D: 0.1247 Loss_G: 6.7639\n",
      "[0/25][75/782] Loss_D: 0.4090 Loss_G: 8.3035\n",
      "[0/25][76/782] Loss_D: 0.1603 Loss_G: 8.4495\n",
      "[0/25][77/782] Loss_D: 0.3165 Loss_G: 9.2020\n",
      "[0/25][78/782] Loss_D: 0.4520 Loss_G: 5.8847\n",
      "[0/25][79/782] Loss_D: 0.4143 Loss_G: 7.4563\n",
      "[0/25][80/782] Loss_D: 0.4061 Loss_G: 7.2790\n",
      "[0/25][81/782] Loss_D: 0.4646 Loss_G: 6.4826\n",
      "[0/25][82/782] Loss_D: 0.4645 Loss_G: 8.3522\n",
      "[0/25][83/782] Loss_D: 0.4782 Loss_G: 5.6754\n",
      "[0/25][84/782] Loss_D: 0.8894 Loss_G: 12.1914\n",
      "[0/25][85/782] Loss_D: 1.4408 Loss_G: 7.3431\n",
      "[0/25][86/782] Loss_D: 0.5679 Loss_G: 5.4966\n",
      "[0/25][87/782] Loss_D: 0.4829 Loss_G: 9.4939\n",
      "[0/25][88/782] Loss_D: 0.6165 Loss_G: 6.1828\n",
      "[0/25][89/782] Loss_D: 0.3182 Loss_G: 6.8378\n",
      "[0/25][90/782] Loss_D: 0.3475 Loss_G: 4.8385\n",
      "[0/25][91/782] Loss_D: 0.4424 Loss_G: 8.2588\n",
      "[0/25][92/782] Loss_D: 0.3116 Loss_G: 5.6826\n",
      "[0/25][93/782] Loss_D: 0.4526 Loss_G: 6.2614\n",
      "[0/25][94/782] Loss_D: 0.2476 Loss_G: 5.9224\n",
      "[0/25][95/782] Loss_D: 0.3652 Loss_G: 4.2121\n",
      "[0/25][96/782] Loss_D: 0.6257 Loss_G: 10.6862\n",
      "[0/25][97/782] Loss_D: 2.8099 Loss_G: 0.8478\n",
      "[0/25][98/782] Loss_D: 2.6316 Loss_G: 9.8080\n",
      "[0/25][99/782] Loss_D: 2.1816 Loss_G: 6.3276\n",
      "[0/25][100/782] Loss_D: 0.2449 Loss_G: 3.2062\n",
      "[0/25][101/782] Loss_D: 0.5473 Loss_G: 5.6885\n",
      "[0/25][102/782] Loss_D: 0.2468 Loss_G: 5.2512\n",
      "[0/25][103/782] Loss_D: 0.3252 Loss_G: 3.6498\n",
      "[0/25][104/782] Loss_D: 0.3395 Loss_G: 3.9648\n",
      "[0/25][105/782] Loss_D: 0.3789 Loss_G: 4.8763\n",
      "[0/25][106/782] Loss_D: 0.6227 Loss_G: 3.1881\n",
      "[0/25][107/782] Loss_D: 0.5946 Loss_G: 4.6457\n",
      "[0/25][108/782] Loss_D: 0.4451 Loss_G: 3.7515\n",
      "[0/25][109/782] Loss_D: 0.3560 Loss_G: 3.5732\n",
      "[0/25][110/782] Loss_D: 0.4038 Loss_G: 3.8696\n",
      "[0/25][111/782] Loss_D: 0.4370 Loss_G: 4.5226\n",
      "[0/25][112/782] Loss_D: 0.5580 Loss_G: 3.9870\n",
      "[0/25][113/782] Loss_D: 0.4762 Loss_G: 4.4472\n",
      "[0/25][114/782] Loss_D: 0.4512 Loss_G: 3.6512\n",
      "[0/25][115/782] Loss_D: 0.6038 Loss_G: 6.2369\n",
      "[0/25][116/782] Loss_D: 0.6998 Loss_G: 2.9410\n",
      "[0/25][117/782] Loss_D: 0.7675 Loss_G: 8.0017\n",
      "[0/25][118/782] Loss_D: 1.1761 Loss_G: 4.3853\n",
      "[0/25][119/782] Loss_D: 0.4830 Loss_G: 3.2177\n",
      "[0/25][120/782] Loss_D: 0.7988 Loss_G: 6.0988\n",
      "[0/25][121/782] Loss_D: 0.7366 Loss_G: 4.2028\n",
      "[0/25][122/782] Loss_D: 0.2968 Loss_G: 3.5978\n",
      "[0/25][123/782] Loss_D: 0.3378 Loss_G: 4.5225\n",
      "[0/25][124/782] Loss_D: 0.2121 Loss_G: 4.2857\n",
      "[0/25][125/782] Loss_D: 0.3227 Loss_G: 3.6476\n",
      "[0/25][126/782] Loss_D: 0.3920 Loss_G: 4.4993\n",
      "[0/25][127/782] Loss_D: 0.3147 Loss_G: 3.8786\n",
      "[0/25][128/782] Loss_D: 0.4912 Loss_G: 3.6564\n",
      "[0/25][129/782] Loss_D: 0.4523 Loss_G: 4.1834\n",
      "[0/25][130/782] Loss_D: 0.4358 Loss_G: 3.7276\n",
      "[0/25][131/782] Loss_D: 0.6797 Loss_G: 4.3803\n",
      "[0/25][132/782] Loss_D: 0.4774 Loss_G: 4.9308\n",
      "[0/25][133/782] Loss_D: 0.4739 Loss_G: 3.3023\n",
      "[0/25][134/782] Loss_D: 0.5872 Loss_G: 7.7681\n",
      "[0/25][135/782] Loss_D: 0.6014 Loss_G: 3.6801\n",
      "[0/25][136/782] Loss_D: 0.4556 Loss_G: 5.1391\n",
      "[0/25][137/782] Loss_D: 0.2898 Loss_G: 5.1838\n",
      "[0/25][138/782] Loss_D: 0.3275 Loss_G: 4.2680\n",
      "[0/25][139/782] Loss_D: 0.4480 Loss_G: 5.9622\n",
      "[0/25][140/782] Loss_D: 0.5415 Loss_G: 2.6552\n",
      "[0/25][141/782] Loss_D: 0.7224 Loss_G: 8.0080\n",
      "[0/25][142/782] Loss_D: 1.4573 Loss_G: 2.0829\n",
      "[0/25][143/782] Loss_D: 1.1887 Loss_G: 9.3533\n",
      "[0/25][144/782] Loss_D: 1.7148 Loss_G: 3.3563\n",
      "[0/25][145/782] Loss_D: 0.8380 Loss_G: 5.3127\n",
      "[0/25][146/782] Loss_D: 0.3329 Loss_G: 5.4742\n",
      "[0/25][147/782] Loss_D: 0.4264 Loss_G: 3.8779\n",
      "[0/25][148/782] Loss_D: 0.5476 Loss_G: 5.2298\n",
      "[0/25][149/782] Loss_D: 0.6103 Loss_G: 3.4093\n",
      "[0/25][150/782] Loss_D: 0.5489 Loss_G: 6.9967\n",
      "[0/25][151/782] Loss_D: 0.4400 Loss_G: 4.4983\n",
      "[0/25][152/782] Loss_D: 0.2648 Loss_G: 3.8606\n",
      "[0/25][153/782] Loss_D: 0.4477 Loss_G: 6.5648\n",
      "[0/25][154/782] Loss_D: 0.7141 Loss_G: 2.6905\n",
      "[0/25][155/782] Loss_D: 1.0408 Loss_G: 9.0498\n",
      "[0/25][156/782] Loss_D: 1.4989 Loss_G: 4.1389\n",
      "[0/25][157/782] Loss_D: 0.4856 Loss_G: 5.0384\n",
      "[0/25][158/782] Loss_D: 0.3219 Loss_G: 5.6840\n",
      "[0/25][159/782] Loss_D: 0.4057 Loss_G: 4.0240\n",
      "[0/25][160/782] Loss_D: 0.5371 Loss_G: 6.2747\n",
      "[0/25][161/782] Loss_D: 0.3291 Loss_G: 4.3296\n",
      "[0/25][162/782] Loss_D: 0.2960 Loss_G: 4.9339\n",
      "[0/25][163/782] Loss_D: 0.3628 Loss_G: 5.4824\n",
      "[0/25][164/782] Loss_D: 0.4624 Loss_G: 3.2910\n",
      "[0/25][165/782] Loss_D: 0.7103 Loss_G: 9.3503\n",
      "[0/25][166/782] Loss_D: 0.5437 Loss_G: 5.8799\n",
      "[0/25][167/782] Loss_D: 0.2520 Loss_G: 3.3489\n",
      "[0/25][168/782] Loss_D: 1.2061 Loss_G: 11.2936\n",
      "[0/25][169/782] Loss_D: 1.2790 Loss_G: 7.6735\n",
      "[0/25][170/782] Loss_D: 0.1187 Loss_G: 3.7858\n",
      "[0/25][171/782] Loss_D: 0.9225 Loss_G: 9.8736\n",
      "[0/25][172/782] Loss_D: 0.5900 Loss_G: 7.3781\n",
      "[0/25][173/782] Loss_D: 0.3260 Loss_G: 3.7415\n",
      "[0/25][174/782] Loss_D: 0.6414 Loss_G: 6.0481\n",
      "[0/25][175/782] Loss_D: 0.1754 Loss_G: 5.8606\n",
      "[0/25][176/782] Loss_D: 0.2687 Loss_G: 3.9959\n",
      "[0/25][177/782] Loss_D: 0.4902 Loss_G: 5.1426\n",
      "[0/25][178/782] Loss_D: 0.3408 Loss_G: 4.5155\n",
      "[0/25][179/782] Loss_D: 0.2803 Loss_G: 4.7874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][180/782] Loss_D: 0.2561 Loss_G: 4.9087\n",
      "[0/25][181/782] Loss_D: 0.3121 Loss_G: 4.4821\n",
      "[0/25][182/782] Loss_D: 0.4487 Loss_G: 5.7360\n",
      "[0/25][183/782] Loss_D: 0.2215 Loss_G: 5.2288\n",
      "[0/25][184/782] Loss_D: 0.2085 Loss_G: 4.8589\n",
      "[0/25][185/782] Loss_D: 0.3274 Loss_G: 4.5996\n",
      "[0/25][186/782] Loss_D: 0.5355 Loss_G: 8.4879\n",
      "[0/25][187/782] Loss_D: 0.2839 Loss_G: 7.1639\n",
      "[0/25][188/782] Loss_D: 0.1226 Loss_G: 4.6807\n",
      "[0/25][189/782] Loss_D: 0.3505 Loss_G: 8.5466\n",
      "[0/25][190/782] Loss_D: 0.4403 Loss_G: 5.8916\n",
      "[0/25][191/782] Loss_D: 0.1758 Loss_G: 5.6044\n",
      "[0/25][192/782] Loss_D: 0.2526 Loss_G: 7.1248\n",
      "[0/25][193/782] Loss_D: 0.1720 Loss_G: 6.1010\n",
      "[0/25][194/782] Loss_D: 0.1929 Loss_G: 5.9900\n",
      "[0/25][195/782] Loss_D: 0.4476 Loss_G: 6.4023\n",
      "[0/25][196/782] Loss_D: 0.2796 Loss_G: 5.6890\n",
      "[0/25][197/782] Loss_D: 0.1510 Loss_G: 7.5480\n",
      "[0/25][198/782] Loss_D: 0.2137 Loss_G: 5.7095\n",
      "[0/25][199/782] Loss_D: 0.1728 Loss_G: 7.4634\n",
      "[0/25][200/782] Loss_D: 0.1580 Loss_G: 6.2937\n",
      "[0/25][201/782] Loss_D: 0.2066 Loss_G: 7.7896\n",
      "[0/25][202/782] Loss_D: 0.1437 Loss_G: 6.7483\n",
      "[0/25][203/782] Loss_D: 0.4083 Loss_G: 6.7200\n",
      "[0/25][204/782] Loss_D: 0.2934 Loss_G: 7.5158\n",
      "[0/25][205/782] Loss_D: 0.0822 Loss_G: 7.0779\n",
      "[0/25][206/782] Loss_D: 0.1096 Loss_G: 6.2112\n",
      "[0/25][207/782] Loss_D: 0.2510 Loss_G: 7.3893\n",
      "[0/25][208/782] Loss_D: 0.1218 Loss_G: 6.8381\n",
      "[0/25][209/782] Loss_D: 0.1753 Loss_G: 5.4500\n",
      "[0/25][210/782] Loss_D: 0.2401 Loss_G: 8.8459\n",
      "[0/25][211/782] Loss_D: 0.0548 Loss_G: 8.6364\n",
      "[0/25][212/782] Loss_D: 0.3479 Loss_G: 4.2871\n",
      "[0/25][213/782] Loss_D: 0.6455 Loss_G: 14.7186\n",
      "[0/25][214/782] Loss_D: 1.0671 Loss_G: 11.5083\n",
      "[0/25][215/782] Loss_D: 0.0790 Loss_G: 7.1586\n",
      "[0/25][216/782] Loss_D: 0.1760 Loss_G: 6.0747\n",
      "[0/25][217/782] Loss_D: 0.2261 Loss_G: 8.6905\n",
      "[0/25][218/782] Loss_D: 0.0357 Loss_G: 8.4655\n",
      "[0/25][219/782] Loss_D: 0.1187 Loss_G: 5.8851\n",
      "[0/25][220/782] Loss_D: 0.1375 Loss_G: 5.2875\n",
      "[0/25][221/782] Loss_D: 0.2731 Loss_G: 7.9034\n",
      "[0/25][222/782] Loss_D: 0.3967 Loss_G: 3.4144\n",
      "[0/25][223/782] Loss_D: 0.7842 Loss_G: 15.5958\n",
      "[0/25][224/782] Loss_D: 2.0668 Loss_G: 8.7454\n",
      "[0/25][225/782] Loss_D: 0.0644 Loss_G: 5.1504\n",
      "[0/25][226/782] Loss_D: 0.8444 Loss_G: 10.7675\n",
      "[0/25][227/782] Loss_D: 1.5038 Loss_G: 6.2680\n",
      "[0/25][228/782] Loss_D: 0.1549 Loss_G: 5.0705\n",
      "[0/25][229/782] Loss_D: 0.1740 Loss_G: 5.8592\n",
      "[0/25][230/782] Loss_D: 0.1676 Loss_G: 5.3447\n",
      "[0/25][231/782] Loss_D: 0.2276 Loss_G: 4.6145\n",
      "[0/25][232/782] Loss_D: 0.2442 Loss_G: 6.0208\n",
      "[0/25][233/782] Loss_D: 0.1660 Loss_G: 5.6847\n",
      "[0/25][234/782] Loss_D: 0.2477 Loss_G: 5.2757\n",
      "[0/25][235/782] Loss_D: 0.5230 Loss_G: 13.6531\n",
      "[0/25][236/782] Loss_D: 3.7655 Loss_G: 3.3892\n",
      "[0/25][237/782] Loss_D: 1.5408 Loss_G: 13.2265\n",
      "[0/25][238/782] Loss_D: 2.6471 Loss_G: 5.2183\n",
      "[0/25][239/782] Loss_D: 0.6423 Loss_G: 7.5704\n",
      "[0/25][240/782] Loss_D: 0.0924 Loss_G: 6.9281\n",
      "[0/25][241/782] Loss_D: 0.2230 Loss_G: 7.0201\n",
      "[0/25][242/782] Loss_D: 0.4123 Loss_G: 5.2345\n",
      "[0/25][243/782] Loss_D: 0.4137 Loss_G: 5.0329\n",
      "[0/25][244/782] Loss_D: 0.2854 Loss_G: 6.3454\n",
      "[0/25][245/782] Loss_D: 0.1378 Loss_G: 6.1600\n",
      "[0/25][246/782] Loss_D: 0.3594 Loss_G: 4.8750\n",
      "[0/25][247/782] Loss_D: 0.2377 Loss_G: 5.8100\n",
      "[0/25][248/782] Loss_D: 0.4277 Loss_G: 4.8333\n",
      "[0/25][249/782] Loss_D: 0.3972 Loss_G: 6.0257\n",
      "[0/25][250/782] Loss_D: 0.1694 Loss_G: 6.4296\n",
      "[0/25][251/782] Loss_D: 0.3271 Loss_G: 4.4039\n",
      "[0/25][252/782] Loss_D: 0.4470 Loss_G: 10.7837\n",
      "[0/25][253/782] Loss_D: 0.2576 Loss_G: 10.3992\n",
      "[0/25][254/782] Loss_D: 0.3054 Loss_G: 6.9660\n",
      "[0/25][255/782] Loss_D: 0.0913 Loss_G: 4.3026\n",
      "[0/25][256/782] Loss_D: 0.5599 Loss_G: 10.4871\n",
      "[0/25][257/782] Loss_D: 0.0658 Loss_G: 11.5166\n",
      "[0/25][258/782] Loss_D: 0.2481 Loss_G: 8.5852\n",
      "[0/25][259/782] Loss_D: 0.0573 Loss_G: 5.1478\n",
      "[0/25][260/782] Loss_D: 0.9712 Loss_G: 16.0262\n",
      "[0/25][261/782] Loss_D: 1.4551 Loss_G: 13.5294\n",
      "[0/25][262/782] Loss_D: 0.1667 Loss_G: 8.2766\n",
      "[0/25][263/782] Loss_D: 0.3555 Loss_G: 8.5283\n",
      "[0/25][264/782] Loss_D: 0.1663 Loss_G: 8.5805\n",
      "[0/25][265/782] Loss_D: 0.1441 Loss_G: 6.6736\n",
      "[0/25][266/782] Loss_D: 0.4311 Loss_G: 5.9944\n",
      "[0/25][267/782] Loss_D: 0.3448 Loss_G: 5.9296\n",
      "[0/25][268/782] Loss_D: 0.3599 Loss_G: 4.8962\n",
      "[0/25][269/782] Loss_D: 0.2114 Loss_G: 5.9042\n",
      "[0/25][270/782] Loss_D: 0.2186 Loss_G: 5.1937\n",
      "[0/25][271/782] Loss_D: 0.2127 Loss_G: 5.1364\n",
      "[0/25][272/782] Loss_D: 0.1622 Loss_G: 5.4094\n",
      "[0/25][273/782] Loss_D: 0.0829 Loss_G: 5.6423\n",
      "[0/25][274/782] Loss_D: 0.0865 Loss_G: 5.5125\n",
      "[0/25][275/782] Loss_D: 0.0982 Loss_G: 5.3083\n",
      "[0/25][276/782] Loss_D: 0.1491 Loss_G: 4.9947\n",
      "[0/25][277/782] Loss_D: 0.1816 Loss_G: 5.4273\n",
      "[0/25][278/782] Loss_D: 0.0718 Loss_G: 5.4229\n",
      "[0/25][279/782] Loss_D: 0.1336 Loss_G: 6.8758\n",
      "[0/25][280/782] Loss_D: 0.1557 Loss_G: 6.1016\n",
      "[0/25][281/782] Loss_D: 0.0822 Loss_G: 5.1393\n",
      "[0/25][282/782] Loss_D: 0.0699 Loss_G: 5.6159\n",
      "[0/25][283/782] Loss_D: 0.1010 Loss_G: 6.2268\n",
      "[0/25][284/782] Loss_D: 0.1799 Loss_G: 4.4509\n",
      "[0/25][285/782] Loss_D: 0.2383 Loss_G: 6.5847\n",
      "[0/25][286/782] Loss_D: 0.1826 Loss_G: 5.3894\n",
      "[0/25][287/782] Loss_D: 0.1530 Loss_G: 6.0258\n",
      "[0/25][288/782] Loss_D: 0.1719 Loss_G: 6.5111\n",
      "[0/25][289/782] Loss_D: 0.2498 Loss_G: 4.9785\n",
      "[0/25][290/782] Loss_D: 0.2349 Loss_G: 9.6186\n",
      "[0/25][291/782] Loss_D: 0.3292 Loss_G: 7.1722\n",
      "[0/25][292/782] Loss_D: 0.1599 Loss_G: 6.3449\n",
      "[0/25][293/782] Loss_D: 0.2335 Loss_G: 8.1754\n",
      "[0/25][294/782] Loss_D: 0.0725 Loss_G: 7.3585\n",
      "[0/25][295/782] Loss_D: 0.2638 Loss_G: 4.6808\n",
      "[0/25][296/782] Loss_D: 0.7276 Loss_G: 15.4066\n",
      "[0/25][297/782] Loss_D: 0.8647 Loss_G: 13.5889\n",
      "[0/25][298/782] Loss_D: 0.4542 Loss_G: 7.9584\n",
      "[0/25][299/782] Loss_D: 0.2283 Loss_G: 6.8236\n",
      "[0/25][300/782] Loss_D: 0.3463 Loss_G: 10.1993\n",
      "[0/25][301/782] Loss_D: 0.2559 Loss_G: 7.5587\n",
      "[0/25][302/782] Loss_D: 0.1974 Loss_G: 6.1896\n",
      "[0/25][303/782] Loss_D: 0.5540 Loss_G: 8.3918\n",
      "[0/25][304/782] Loss_D: 0.6946 Loss_G: 3.0292\n",
      "[0/25][305/782] Loss_D: 1.5490 Loss_G: 17.5049\n",
      "[0/25][306/782] Loss_D: 4.3684 Loss_G: 11.6700\n",
      "[0/25][307/782] Loss_D: 0.0933 Loss_G: 6.0551\n",
      "[0/25][308/782] Loss_D: 0.4430 Loss_G: 7.6113\n",
      "[0/25][309/782] Loss_D: 0.1023 Loss_G: 7.4675\n",
      "[0/25][310/782] Loss_D: 0.2236 Loss_G: 6.3083\n",
      "[0/25][311/782] Loss_D: 0.2137 Loss_G: 5.1172\n",
      "[0/25][312/782] Loss_D: 0.2293 Loss_G: 5.2753\n",
      "[0/25][313/782] Loss_D: 0.1986 Loss_G: 5.2651\n",
      "[0/25][314/782] Loss_D: 0.2742 Loss_G: 5.3497\n",
      "[0/25][315/782] Loss_D: 0.1258 Loss_G: 5.5156\n",
      "[0/25][316/782] Loss_D: 0.2162 Loss_G: 4.4237\n",
      "[0/25][317/782] Loss_D: 0.2966 Loss_G: 6.6397\n",
      "[0/25][318/782] Loss_D: 0.0962 Loss_G: 6.5092\n",
      "[0/25][319/782] Loss_D: 0.1380 Loss_G: 4.8305\n",
      "[0/25][320/782] Loss_D: 0.4152 Loss_G: 6.3961\n",
      "[0/25][321/782] Loss_D: 0.1419 Loss_G: 5.7041\n",
      "[0/25][322/782] Loss_D: 0.3298 Loss_G: 6.1436\n",
      "[0/25][323/782] Loss_D: 0.2337 Loss_G: 4.7648\n",
      "[0/25][324/782] Loss_D: 0.4079 Loss_G: 9.4206\n",
      "[0/25][325/782] Loss_D: 0.6453 Loss_G: 7.0633\n",
      "[0/25][326/782] Loss_D: 0.0700 Loss_G: 4.7995\n",
      "[0/25][327/782] Loss_D: 0.1361 Loss_G: 5.9465\n",
      "[0/25][328/782] Loss_D: 0.0464 Loss_G: 6.0831\n",
      "[0/25][329/782] Loss_D: 0.0903 Loss_G: 6.0718\n",
      "[0/25][330/782] Loss_D: 0.1026 Loss_G: 6.0203\n",
      "[0/25][331/782] Loss_D: 0.1772 Loss_G: 5.3401\n",
      "[0/25][332/782] Loss_D: 0.1222 Loss_G: 5.8439\n",
      "[0/25][333/782] Loss_D: 0.2540 Loss_G: 5.6580\n",
      "[0/25][334/782] Loss_D: 0.3647 Loss_G: 6.9826\n",
      "[0/25][335/782] Loss_D: 0.1370 Loss_G: 6.6045\n",
      "[0/25][336/782] Loss_D: 0.2501 Loss_G: 6.2695\n",
      "[0/25][337/782] Loss_D: 0.3515 Loss_G: 9.6432\n",
      "[0/25][338/782] Loss_D: 0.2361 Loss_G: 7.9045\n",
      "[0/25][339/782] Loss_D: 0.1777 Loss_G: 5.1157\n",
      "[0/25][340/782] Loss_D: 0.4698 Loss_G: 10.6407\n",
      "[0/25][341/782] Loss_D: 0.1870 Loss_G: 10.6457\n",
      "[0/25][342/782] Loss_D: 0.3706 Loss_G: 5.7201\n",
      "[0/25][343/782] Loss_D: 0.1853 Loss_G: 6.1645\n",
      "[0/25][344/782] Loss_D: 0.1165 Loss_G: 7.5097\n",
      "[0/25][345/782] Loss_D: 0.1355 Loss_G: 6.8544\n",
      "[0/25][346/782] Loss_D: 0.1314 Loss_G: 6.2402\n",
      "[0/25][347/782] Loss_D: 0.0903 Loss_G: 5.8549\n",
      "[0/25][348/782] Loss_D: 0.2000 Loss_G: 7.6471\n",
      "[0/25][349/782] Loss_D: 0.1825 Loss_G: 6.2663\n",
      "[0/25][350/782] Loss_D: 0.3334 Loss_G: 5.8883\n",
      "[0/25][351/782] Loss_D: 0.3028 Loss_G: 9.0268\n",
      "[0/25][352/782] Loss_D: 0.3853 Loss_G: 6.7215\n",
      "[0/25][353/782] Loss_D: 0.2795 Loss_G: 4.1590\n",
      "[0/25][354/782] Loss_D: 0.3773 Loss_G: 9.2409\n",
      "[0/25][355/782] Loss_D: 0.1558 Loss_G: 8.6346\n",
      "[0/25][356/782] Loss_D: 0.1540 Loss_G: 6.5317\n",
      "[0/25][357/782] Loss_D: 0.0564 Loss_G: 4.7298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][358/782] Loss_D: 0.1476 Loss_G: 5.6503\n",
      "[0/25][359/782] Loss_D: 0.0757 Loss_G: 6.1488\n",
      "[0/25][360/782] Loss_D: 0.1568 Loss_G: 5.3165\n",
      "[0/25][361/782] Loss_D: 0.0920 Loss_G: 5.2855\n",
      "[0/25][362/782] Loss_D: 0.1487 Loss_G: 5.9162\n",
      "[0/25][363/782] Loss_D: 0.0921 Loss_G: 5.8038\n",
      "[0/25][364/782] Loss_D: 0.1671 Loss_G: 5.8410\n",
      "[0/25][365/782] Loss_D: 0.1064 Loss_G: 6.3320\n",
      "[0/25][366/782] Loss_D: 0.1478 Loss_G: 5.7936\n",
      "[0/25][367/782] Loss_D: 0.1375 Loss_G: 6.9008\n",
      "[0/25][368/782] Loss_D: 0.0545 Loss_G: 6.7630\n",
      "[0/25][369/782] Loss_D: 0.1179 Loss_G: 6.0058\n",
      "[0/25][370/782] Loss_D: 0.1235 Loss_G: 5.3580\n",
      "[0/25][371/782] Loss_D: 0.2148 Loss_G: 6.1477\n",
      "[0/25][372/782] Loss_D: 0.1089 Loss_G: 5.8225\n",
      "[0/25][373/782] Loss_D: 0.0921 Loss_G: 5.6247\n",
      "[0/25][374/782] Loss_D: 0.0891 Loss_G: 5.8649\n",
      "[0/25][375/782] Loss_D: 0.0853 Loss_G: 5.5349\n",
      "[0/25][376/782] Loss_D: 0.0926 Loss_G: 5.0685\n",
      "[0/25][377/782] Loss_D: 0.0718 Loss_G: 6.1748\n",
      "[0/25][378/782] Loss_D: 0.1214 Loss_G: 5.3121\n",
      "[0/25][379/782] Loss_D: 0.0740 Loss_G: 5.6391\n",
      "[0/25][380/782] Loss_D: 0.0657 Loss_G: 5.4078\n",
      "[0/25][381/782] Loss_D: 0.1344 Loss_G: 6.5329\n",
      "[0/25][382/782] Loss_D: 0.0359 Loss_G: 6.7783\n",
      "[0/25][383/782] Loss_D: 0.0641 Loss_G: 6.4314\n",
      "[0/25][384/782] Loss_D: 0.2012 Loss_G: 8.7798\n",
      "[0/25][385/782] Loss_D: 0.1265 Loss_G: 8.9287\n",
      "[0/25][386/782] Loss_D: 0.5814 Loss_G: 6.9147\n",
      "[0/25][387/782] Loss_D: 0.3941 Loss_G: 14.9338\n",
      "[0/25][388/782] Loss_D: 0.2633 Loss_G: 13.7195\n",
      "[0/25][389/782] Loss_D: 0.0710 Loss_G: 8.4404\n",
      "[0/25][390/782] Loss_D: 0.2978 Loss_G: 9.1390\n",
      "[0/25][391/782] Loss_D: 0.0969 Loss_G: 8.8971\n",
      "[0/25][392/782] Loss_D: 0.1353 Loss_G: 6.8180\n",
      "[0/25][393/782] Loss_D: 0.1907 Loss_G: 6.5699\n",
      "[0/25][394/782] Loss_D: 0.4064 Loss_G: 10.5172\n",
      "[0/25][395/782] Loss_D: 0.3737 Loss_G: 6.7974\n",
      "[0/25][396/782] Loss_D: 0.1979 Loss_G: 6.7402\n",
      "[0/25][397/782] Loss_D: 0.1234 Loss_G: 7.7260\n",
      "[0/25][398/782] Loss_D: 0.2198 Loss_G: 5.8448\n",
      "[0/25][399/782] Loss_D: 0.1524 Loss_G: 7.6162\n",
      "[0/25][400/782] Loss_D: 0.0457 Loss_G: 7.3199\n",
      "[0/25][401/782] Loss_D: 0.1662 Loss_G: 4.7424\n",
      "[0/25][402/782] Loss_D: 0.4372 Loss_G: 13.7657\n",
      "[0/25][403/782] Loss_D: 1.0999 Loss_G: 9.1870\n",
      "[0/25][404/782] Loss_D: 0.0481 Loss_G: 5.5799\n",
      "[0/25][405/782] Loss_D: 0.3336 Loss_G: 9.2175\n",
      "[0/25][406/782] Loss_D: 0.2576 Loss_G: 7.4187\n",
      "[0/25][407/782] Loss_D: 0.2803 Loss_G: 9.1793\n",
      "[0/25][408/782] Loss_D: 0.1151 Loss_G: 7.5705\n",
      "[0/25][409/782] Loss_D: 0.1237 Loss_G: 6.3390\n",
      "[0/25][410/782] Loss_D: 0.1621 Loss_G: 7.8898\n",
      "[0/25][411/782] Loss_D: 0.0750 Loss_G: 7.1246\n",
      "[0/25][412/782] Loss_D: 0.1297 Loss_G: 6.2124\n",
      "[0/25][413/782] Loss_D: 0.1639 Loss_G: 9.0713\n",
      "[0/25][414/782] Loss_D: 0.0667 Loss_G: 8.2681\n",
      "[0/25][415/782] Loss_D: 0.1318 Loss_G: 6.6227\n",
      "[0/25][416/782] Loss_D: 0.2705 Loss_G: 10.9751\n",
      "[0/25][417/782] Loss_D: 0.4501 Loss_G: 6.8433\n",
      "[0/25][418/782] Loss_D: 0.4830 Loss_G: 15.9939\n",
      "[0/25][419/782] Loss_D: 0.3913 Loss_G: 14.3483\n",
      "[0/25][420/782] Loss_D: 0.0896 Loss_G: 10.1231\n",
      "[0/25][421/782] Loss_D: 0.4414 Loss_G: 11.1308\n",
      "[0/25][422/782] Loss_D: 0.1688 Loss_G: 9.9283\n",
      "[0/25][423/782] Loss_D: 0.1510 Loss_G: 9.2135\n",
      "[0/25][424/782] Loss_D: 0.2969 Loss_G: 5.7671\n",
      "[0/25][425/782] Loss_D: 0.2659 Loss_G: 11.7592\n",
      "[0/25][426/782] Loss_D: 0.0294 Loss_G: 12.4693\n",
      "[0/25][427/782] Loss_D: 0.0373 Loss_G: 11.0114\n",
      "[0/25][428/782] Loss_D: 0.1061 Loss_G: 7.3741\n",
      "[0/25][429/782] Loss_D: 0.0634 Loss_G: 5.0419\n",
      "[0/25][430/782] Loss_D: 0.2510 Loss_G: 10.0162\n",
      "[0/25][431/782] Loss_D: 0.0830 Loss_G: 10.0072\n",
      "[0/25][432/782] Loss_D: 0.0946 Loss_G: 7.6659\n",
      "[0/25][433/782] Loss_D: 0.0685 Loss_G: 4.7742\n",
      "[0/25][434/782] Loss_D: 0.2526 Loss_G: 10.3124\n",
      "[0/25][435/782] Loss_D: 0.1920 Loss_G: 9.3043\n",
      "[0/25][436/782] Loss_D: 0.0747 Loss_G: 6.4638\n",
      "[0/25][437/782] Loss_D: 0.1073 Loss_G: 5.7110\n",
      "[0/25][438/782] Loss_D: 0.4572 Loss_G: 13.9710\n",
      "[0/25][439/782] Loss_D: 1.2487 Loss_G: 8.0847\n",
      "[0/25][440/782] Loss_D: 0.0258 Loss_G: 4.9904\n",
      "[0/25][441/782] Loss_D: 0.4825 Loss_G: 13.5577\n",
      "[0/25][442/782] Loss_D: 0.6980 Loss_G: 11.0223\n",
      "[0/25][443/782] Loss_D: 0.0751 Loss_G: 8.2630\n",
      "[0/25][444/782] Loss_D: 0.0628 Loss_G: 7.1769\n",
      "[0/25][445/782] Loss_D: 0.0206 Loss_G: 6.3978\n",
      "[0/25][446/782] Loss_D: 0.1153 Loss_G: 7.1385\n",
      "[0/25][447/782] Loss_D: 0.0413 Loss_G: 7.1084\n",
      "[0/25][448/782] Loss_D: 0.0708 Loss_G: 6.0517\n",
      "[0/25][449/782] Loss_D: 0.1074 Loss_G: 7.2129\n",
      "[0/25][450/782] Loss_D: 0.1196 Loss_G: 6.2076\n",
      "[0/25][451/782] Loss_D: 0.1793 Loss_G: 6.0655\n",
      "[0/25][452/782] Loss_D: 0.1547 Loss_G: 7.3317\n",
      "[0/25][453/782] Loss_D: 0.3583 Loss_G: 3.9455\n",
      "[0/25][454/782] Loss_D: 0.4378 Loss_G: 14.1264\n",
      "[0/25][455/782] Loss_D: 0.8398 Loss_G: 11.6663\n",
      "[0/25][456/782] Loss_D: 0.0085 Loss_G: 9.4526\n",
      "[0/25][457/782] Loss_D: 0.0158 Loss_G: 7.0263\n",
      "[0/25][458/782] Loss_D: 0.1546 Loss_G: 7.6057\n",
      "[0/25][459/782] Loss_D: 0.0653 Loss_G: 7.8604\n",
      "[0/25][460/782] Loss_D: 0.3122 Loss_G: 8.1254\n",
      "[0/25][461/782] Loss_D: 0.1963 Loss_G: 7.5045\n",
      "[0/25][462/782] Loss_D: 0.3148 Loss_G: 8.2375\n",
      "[0/25][463/782] Loss_D: 0.2283 Loss_G: 6.8156\n",
      "[0/25][464/782] Loss_D: 0.2058 Loss_G: 6.9906\n",
      "[0/25][465/782] Loss_D: 0.3257 Loss_G: 7.2975\n",
      "[0/25][466/782] Loss_D: 0.1772 Loss_G: 6.1936\n",
      "[0/25][467/782] Loss_D: 0.2630 Loss_G: 8.2123\n",
      "[0/25][468/782] Loss_D: 0.1369 Loss_G: 7.3525\n",
      "[0/25][469/782] Loss_D: 0.1464 Loss_G: 5.5020\n",
      "[0/25][470/782] Loss_D: 0.3097 Loss_G: 4.5662\n",
      "[0/25][471/782] Loss_D: 0.2411 Loss_G: 7.7764\n",
      "[0/25][472/782] Loss_D: 0.3464 Loss_G: 6.0327\n",
      "[0/25][473/782] Loss_D: 0.0919 Loss_G: 4.4997\n",
      "[0/25][474/782] Loss_D: 0.2722 Loss_G: 8.7671\n",
      "[0/25][475/782] Loss_D: 0.0964 Loss_G: 8.5500\n",
      "[0/25][476/782] Loss_D: 0.2587 Loss_G: 3.9969\n",
      "[0/25][477/782] Loss_D: 0.2779 Loss_G: 6.8564\n",
      "[0/25][478/782] Loss_D: 0.0747 Loss_G: 7.0012\n",
      "[0/25][479/782] Loss_D: 0.2292 Loss_G: 4.2687\n",
      "[0/25][480/782] Loss_D: 0.2938 Loss_G: 8.0263\n",
      "[0/25][481/782] Loss_D: 0.2291 Loss_G: 6.0342\n",
      "[0/25][482/782] Loss_D: 0.1318 Loss_G: 4.5909\n",
      "[0/25][483/782] Loss_D: 0.1584 Loss_G: 6.0531\n",
      "[0/25][484/782] Loss_D: 0.2040 Loss_G: 5.0653\n",
      "[0/25][485/782] Loss_D: 0.1441 Loss_G: 5.3652\n",
      "[0/25][486/782] Loss_D: 0.1344 Loss_G: 4.9654\n",
      "[0/25][487/782] Loss_D: 0.1492 Loss_G: 5.4810\n",
      "[0/25][488/782] Loss_D: 0.0978 Loss_G: 5.1177\n",
      "[0/25][489/782] Loss_D: 0.0952 Loss_G: 5.1346\n",
      "[0/25][490/782] Loss_D: 0.0995 Loss_G: 6.2215\n",
      "[0/25][491/782] Loss_D: 0.1996 Loss_G: 3.9839\n",
      "[0/25][492/782] Loss_D: 0.3091 Loss_G: 9.6430\n",
      "[0/25][493/782] Loss_D: 0.3029 Loss_G: 7.1730\n",
      "[0/25][494/782] Loss_D: 0.0319 Loss_G: 5.1783\n",
      "[0/25][495/782] Loss_D: 0.0990 Loss_G: 5.6262\n",
      "[0/25][496/782] Loss_D: 0.0904 Loss_G: 6.1872\n",
      "[0/25][497/782] Loss_D: 0.0379 Loss_G: 6.3702\n",
      "[0/25][498/782] Loss_D: 0.1245 Loss_G: 5.1778\n",
      "[0/25][499/782] Loss_D: 0.1550 Loss_G: 4.5998\n",
      "[0/25][500/782] Loss_D: 0.1970 Loss_G: 6.3295\n",
      "[0/25][501/782] Loss_D: 0.1074 Loss_G: 5.9706\n",
      "[0/25][502/782] Loss_D: 0.0977 Loss_G: 5.6663\n",
      "[0/25][503/782] Loss_D: 0.1687 Loss_G: 5.2147\n",
      "[0/25][504/782] Loss_D: 0.1521 Loss_G: 6.7515\n",
      "[0/25][505/782] Loss_D: 0.1724 Loss_G: 4.6655\n",
      "[0/25][506/782] Loss_D: 0.1264 Loss_G: 5.5281\n",
      "[0/25][507/782] Loss_D: 0.1190 Loss_G: 5.2689\n",
      "[0/25][508/782] Loss_D: 0.0572 Loss_G: 6.3511\n",
      "[0/25][509/782] Loss_D: 0.1220 Loss_G: 5.4911\n",
      "[0/25][510/782] Loss_D: 0.3286 Loss_G: 14.8055\n",
      "[0/25][511/782] Loss_D: 1.8216 Loss_G: 7.6350\n",
      "[0/25][512/782] Loss_D: 0.0601 Loss_G: 4.4783\n",
      "[0/25][513/782] Loss_D: 1.2307 Loss_G: 21.7169\n",
      "[0/25][514/782] Loss_D: 1.1838 Loss_G: 21.6517\n",
      "[0/25][515/782] Loss_D: 0.9171 Loss_G: 15.0715\n",
      "[0/25][516/782] Loss_D: 0.1483 Loss_G: 7.0610\n",
      "[0/25][517/782] Loss_D: 1.5650 Loss_G: 14.1434\n",
      "[0/25][518/782] Loss_D: 0.2173 Loss_G: 13.5959\n",
      "[0/25][519/782] Loss_D: 0.3414 Loss_G: 10.4001\n",
      "[0/25][520/782] Loss_D: 0.3093 Loss_G: 6.4284\n",
      "[0/25][521/782] Loss_D: 0.4999 Loss_G: 5.8368\n",
      "[0/25][522/782] Loss_D: 0.7469 Loss_G: 10.4395\n",
      "[0/25][523/782] Loss_D: 0.5074 Loss_G: 8.3959\n",
      "[0/25][524/782] Loss_D: 0.2243 Loss_G: 5.0959\n",
      "[0/25][525/782] Loss_D: 1.5030 Loss_G: 15.3412\n",
      "[0/25][526/782] Loss_D: 3.3288 Loss_G: 9.9982\n",
      "[0/25][527/782] Loss_D: 0.4402 Loss_G: 3.0660\n",
      "[0/25][528/782] Loss_D: 1.5604 Loss_G: 10.3007\n",
      "[0/25][529/782] Loss_D: 0.5567 Loss_G: 9.8396\n",
      "[0/25][530/782] Loss_D: 0.2374 Loss_G: 7.4524\n",
      "[0/25][531/782] Loss_D: 0.1153 Loss_G: 5.6081\n",
      "[0/25][532/782] Loss_D: 0.4340 Loss_G: 7.4480\n",
      "[0/25][533/782] Loss_D: 0.1427 Loss_G: 7.0670\n",
      "[0/25][534/782] Loss_D: 0.2280 Loss_G: 5.8715\n",
      "[0/25][535/782] Loss_D: 0.2048 Loss_G: 4.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][536/782] Loss_D: 0.3646 Loss_G: 9.5599\n",
      "[0/25][537/782] Loss_D: 0.3914 Loss_G: 7.8024\n",
      "[0/25][538/782] Loss_D: 0.2609 Loss_G: 5.2251\n",
      "[0/25][539/782] Loss_D: 0.6158 Loss_G: 9.5913\n",
      "[0/25][540/782] Loss_D: 1.7468 Loss_G: 3.2037\n",
      "[0/25][541/782] Loss_D: 1.4679 Loss_G: 11.7525\n",
      "[0/25][542/782] Loss_D: 0.4254 Loss_G: 12.7723\n",
      "[0/25][543/782] Loss_D: 0.3548 Loss_G: 9.3538\n",
      "[0/25][544/782] Loss_D: 0.1547 Loss_G: 6.2425\n",
      "[0/25][545/782] Loss_D: 0.3235 Loss_G: 5.7829\n",
      "[0/25][546/782] Loss_D: 0.2720 Loss_G: 5.1331\n",
      "[0/25][547/782] Loss_D: 0.2081 Loss_G: 4.9564\n",
      "[0/25][548/782] Loss_D: 0.5209 Loss_G: 6.7756\n",
      "[0/25][549/782] Loss_D: 0.3490 Loss_G: 5.2879\n",
      "[0/25][550/782] Loss_D: 0.2683 Loss_G: 3.6114\n",
      "[0/25][551/782] Loss_D: 0.5350 Loss_G: 6.8188\n",
      "[0/25][552/782] Loss_D: 0.4884 Loss_G: 5.2832\n",
      "[0/25][553/782] Loss_D: 0.1724 Loss_G: 4.2460\n",
      "[0/25][554/782] Loss_D: 0.2677 Loss_G: 6.2087\n",
      "[0/25][555/782] Loss_D: 0.1780 Loss_G: 5.4748\n",
      "[0/25][556/782] Loss_D: 0.1829 Loss_G: 3.7575\n",
      "[0/25][557/782] Loss_D: 0.2278 Loss_G: 5.4831\n",
      "[0/25][558/782] Loss_D: 0.0791 Loss_G: 5.7364\n",
      "[0/25][559/782] Loss_D: 0.2997 Loss_G: 3.3439\n",
      "[0/25][560/782] Loss_D: 0.2608 Loss_G: 7.1701\n",
      "[0/25][561/782] Loss_D: 0.1338 Loss_G: 6.2907\n",
      "[0/25][562/782] Loss_D: 0.3790 Loss_G: 3.2509\n",
      "[0/25][563/782] Loss_D: 0.3881 Loss_G: 7.6358\n",
      "[0/25][564/782] Loss_D: 0.2464 Loss_G: 6.0739\n",
      "[0/25][565/782] Loss_D: 0.1506 Loss_G: 3.9649\n",
      "[0/25][566/782] Loss_D: 0.4686 Loss_G: 8.2917\n",
      "[0/25][567/782] Loss_D: 0.7596 Loss_G: 5.6160\n",
      "[0/25][568/782] Loss_D: 0.2678 Loss_G: 3.7547\n",
      "[0/25][569/782] Loss_D: 0.6360 Loss_G: 10.2924\n",
      "[0/25][570/782] Loss_D: 1.4135 Loss_G: 5.1117\n",
      "[0/25][571/782] Loss_D: 0.4634 Loss_G: 7.2308\n",
      "[0/25][572/782] Loss_D: 0.4666 Loss_G: 5.5441\n",
      "[0/25][573/782] Loss_D: 0.1247 Loss_G: 4.5624\n",
      "[0/25][574/782] Loss_D: 0.1758 Loss_G: 5.4259\n",
      "[0/25][575/782] Loss_D: 0.1128 Loss_G: 5.4121\n",
      "[0/25][576/782] Loss_D: 0.1679 Loss_G: 4.2868\n",
      "[0/25][577/782] Loss_D: 0.1783 Loss_G: 4.9949\n",
      "[0/25][578/782] Loss_D: 0.2408 Loss_G: 4.6451\n",
      "[0/25][579/782] Loss_D: 0.3627 Loss_G: 4.4302\n",
      "[0/25][580/782] Loss_D: 0.5009 Loss_G: 8.5511\n",
      "[0/25][581/782] Loss_D: 0.5351 Loss_G: 6.3767\n",
      "[0/25][582/782] Loss_D: 0.1823 Loss_G: 3.8240\n",
      "[0/25][583/782] Loss_D: 0.4749 Loss_G: 10.0097\n",
      "[0/25][584/782] Loss_D: 0.2477 Loss_G: 9.9248\n",
      "[0/25][585/782] Loss_D: 0.1193 Loss_G: 7.3315\n",
      "[0/25][586/782] Loss_D: 0.0553 Loss_G: 5.0757\n",
      "[0/25][587/782] Loss_D: 0.1489 Loss_G: 5.7623\n",
      "[0/25][588/782] Loss_D: 0.1124 Loss_G: 6.3536\n",
      "[0/25][589/782] Loss_D: 0.0929 Loss_G: 6.6972\n",
      "[0/25][590/782] Loss_D: 0.1108 Loss_G: 6.1825\n",
      "[0/25][591/782] Loss_D: 0.2168 Loss_G: 4.9272\n",
      "[0/25][592/782] Loss_D: 0.3057 Loss_G: 7.6138\n",
      "[0/25][593/782] Loss_D: 0.1586 Loss_G: 7.1579\n",
      "[0/25][594/782] Loss_D: 0.2380 Loss_G: 5.0286\n",
      "[0/25][595/782] Loss_D: 0.2430 Loss_G: 7.3012\n",
      "[0/25][596/782] Loss_D: 0.2157 Loss_G: 5.9979\n",
      "[0/25][597/782] Loss_D: 0.3579 Loss_G: 6.0560\n",
      "[0/25][598/782] Loss_D: 0.4760 Loss_G: 10.0690\n",
      "[0/25][599/782] Loss_D: 0.6965 Loss_G: 6.7267\n",
      "[0/25][600/782] Loss_D: 0.4187 Loss_G: 7.7032\n",
      "[0/25][601/782] Loss_D: 0.1665 Loss_G: 7.2471\n",
      "[0/25][602/782] Loss_D: 0.0757 Loss_G: 6.4280\n",
      "[0/25][603/782] Loss_D: 0.0832 Loss_G: 4.5932\n",
      "[0/25][604/782] Loss_D: 0.1913 Loss_G: 7.4280\n",
      "[0/25][605/782] Loss_D: 0.3245 Loss_G: 5.6070\n",
      "[0/25][606/782] Loss_D: 0.1475 Loss_G: 5.0951\n",
      "[0/25][607/782] Loss_D: 0.2244 Loss_G: 6.8657\n",
      "[0/25][608/782] Loss_D: 0.1592 Loss_G: 5.4683\n",
      "[0/25][609/782] Loss_D: 0.1452 Loss_G: 4.7887\n",
      "[0/25][610/782] Loss_D: 0.1699 Loss_G: 5.8943\n",
      "[0/25][611/782] Loss_D: 0.2584 Loss_G: 3.5246\n",
      "[0/25][612/782] Loss_D: 0.5412 Loss_G: 11.9444\n",
      "[0/25][613/782] Loss_D: 1.3136 Loss_G: 7.3018\n",
      "[0/25][614/782] Loss_D: 0.1644 Loss_G: 3.9938\n",
      "[0/25][615/782] Loss_D: 0.6384 Loss_G: 9.0721\n",
      "[0/25][616/782] Loss_D: 0.1781 Loss_G: 8.1527\n",
      "[0/25][617/782] Loss_D: 0.2801 Loss_G: 4.5238\n",
      "[0/25][618/782] Loss_D: 0.1994 Loss_G: 4.4822\n",
      "[0/25][619/782] Loss_D: 0.2669 Loss_G: 7.3142\n",
      "[0/25][620/782] Loss_D: 0.3718 Loss_G: 4.9640\n",
      "[0/25][621/782] Loss_D: 0.3445 Loss_G: 2.7385\n",
      "[0/25][622/782] Loss_D: 0.6902 Loss_G: 11.7324\n",
      "[0/25][623/782] Loss_D: 0.7990 Loss_G: 10.4266\n",
      "[0/25][624/782] Loss_D: 0.2077 Loss_G: 7.9547\n",
      "[0/25][625/782] Loss_D: 0.1392 Loss_G: 5.3500\n",
      "[0/25][626/782] Loss_D: 0.1148 Loss_G: 4.7071\n",
      "[0/25][627/782] Loss_D: 0.1454 Loss_G: 6.0700\n",
      "[0/25][628/782] Loss_D: 0.1130 Loss_G: 5.8921\n",
      "[0/25][629/782] Loss_D: 0.2208 Loss_G: 4.4194\n",
      "[0/25][630/782] Loss_D: 0.3278 Loss_G: 7.4953\n",
      "[0/25][631/782] Loss_D: 0.2367 Loss_G: 7.1726\n",
      "[0/25][632/782] Loss_D: 0.1542 Loss_G: 4.9037\n",
      "[0/25][633/782] Loss_D: 0.3411 Loss_G: 5.6882\n",
      "[0/25][634/782] Loss_D: 0.2729 Loss_G: 5.4982\n",
      "[0/25][635/782] Loss_D: 0.3036 Loss_G: 5.6617\n",
      "[0/25][636/782] Loss_D: 0.4640 Loss_G: 4.0689\n",
      "[0/25][637/782] Loss_D: 0.5610 Loss_G: 8.5781\n",
      "[0/25][638/782] Loss_D: 0.8194 Loss_G: 4.5235\n",
      "[0/25][639/782] Loss_D: 0.2392 Loss_G: 3.6261\n",
      "[0/25][640/782] Loss_D: 0.3078 Loss_G: 5.5540\n",
      "[0/25][641/782] Loss_D: 0.2606 Loss_G: 4.8966\n",
      "[0/25][642/782] Loss_D: 0.1091 Loss_G: 3.9045\n",
      "[0/25][643/782] Loss_D: 0.3220 Loss_G: 4.2915\n",
      "[0/25][644/782] Loss_D: 0.3720 Loss_G: 3.2598\n",
      "[0/25][645/782] Loss_D: 0.4975 Loss_G: 5.4608\n",
      "[0/25][646/782] Loss_D: 0.2572 Loss_G: 4.5600\n",
      "[0/25][647/782] Loss_D: 0.2723 Loss_G: 2.9552\n",
      "[0/25][648/782] Loss_D: 0.5067 Loss_G: 5.3443\n",
      "[0/25][649/782] Loss_D: 0.5153 Loss_G: 2.9951\n",
      "[0/25][650/782] Loss_D: 0.4221 Loss_G: 3.8631\n",
      "[0/25][651/782] Loss_D: 0.3894 Loss_G: 5.8660\n",
      "[0/25][652/782] Loss_D: 1.1428 Loss_G: 0.7177\n",
      "[0/25][653/782] Loss_D: 2.0213 Loss_G: 9.1073\n",
      "[0/25][654/782] Loss_D: 2.6287 Loss_G: 4.3437\n",
      "[0/25][655/782] Loss_D: 0.2332 Loss_G: 2.6772\n",
      "[0/25][656/782] Loss_D: 0.5364 Loss_G: 3.8256\n",
      "[0/25][657/782] Loss_D: 0.2295 Loss_G: 4.4112\n",
      "[0/25][658/782] Loss_D: 0.3844 Loss_G: 3.1272\n",
      "[0/25][659/782] Loss_D: 0.5148 Loss_G: 2.4967\n",
      "[0/25][660/782] Loss_D: 0.6244 Loss_G: 3.2303\n",
      "[0/25][661/782] Loss_D: 0.4604 Loss_G: 4.0438\n",
      "[0/25][662/782] Loss_D: 0.5196 Loss_G: 2.5852\n",
      "[0/25][663/782] Loss_D: 0.7552 Loss_G: 4.4569\n",
      "[0/25][664/782] Loss_D: 0.1898 Loss_G: 5.2604\n",
      "[0/25][665/782] Loss_D: 0.5378 Loss_G: 2.6474\n",
      "[0/25][666/782] Loss_D: 0.4979 Loss_G: 3.5769\n",
      "[0/25][667/782] Loss_D: 0.5896 Loss_G: 2.9901\n",
      "[0/25][668/782] Loss_D: 0.6309 Loss_G: 5.6500\n",
      "[0/25][669/782] Loss_D: 0.8862 Loss_G: 2.8535\n",
      "[0/25][670/782] Loss_D: 0.5144 Loss_G: 3.7671\n",
      "[0/25][671/782] Loss_D: 0.3113 Loss_G: 3.5297\n",
      "[0/25][672/782] Loss_D: 0.2228 Loss_G: 3.5914\n",
      "[0/25][673/782] Loss_D: 0.3271 Loss_G: 3.2310\n",
      "[0/25][674/782] Loss_D: 0.1855 Loss_G: 4.0570\n",
      "[0/25][675/782] Loss_D: 0.3813 Loss_G: 4.6999\n",
      "[0/25][676/782] Loss_D: 0.3611 Loss_G: 2.9626\n",
      "[0/25][677/782] Loss_D: 0.4216 Loss_G: 5.0724\n",
      "[0/25][678/782] Loss_D: 0.5299 Loss_G: 2.3770\n",
      "[0/25][679/782] Loss_D: 0.4819 Loss_G: 5.1231\n",
      "[0/25][680/782] Loss_D: 0.8090 Loss_G: 1.2141\n",
      "[0/25][681/782] Loss_D: 1.0263 Loss_G: 8.1094\n",
      "[0/25][682/782] Loss_D: 1.4198 Loss_G: 3.6682\n",
      "[0/25][683/782] Loss_D: 0.4309 Loss_G: 3.3497\n",
      "[0/25][684/782] Loss_D: 0.3047 Loss_G: 4.6698\n",
      "[0/25][685/782] Loss_D: 0.2968 Loss_G: 3.9437\n",
      "[0/25][686/782] Loss_D: 0.3174 Loss_G: 3.9672\n",
      "[0/25][687/782] Loss_D: 0.5248 Loss_G: 2.5462\n",
      "[0/25][688/782] Loss_D: 0.6887 Loss_G: 5.4175\n",
      "[0/25][689/782] Loss_D: 1.0149 Loss_G: 1.1977\n",
      "[0/25][690/782] Loss_D: 1.8695 Loss_G: 9.9286\n",
      "[0/25][691/782] Loss_D: 2.4710 Loss_G: 3.4135\n",
      "[0/25][692/782] Loss_D: 0.5366 Loss_G: 2.2998\n",
      "[0/25][693/782] Loss_D: 0.9489 Loss_G: 6.6486\n",
      "[0/25][694/782] Loss_D: 0.9141 Loss_G: 4.4164\n",
      "[0/25][695/782] Loss_D: 0.3591 Loss_G: 3.0029\n",
      "[0/25][696/782] Loss_D: 0.6662 Loss_G: 5.0700\n",
      "[0/25][697/782] Loss_D: 0.7535 Loss_G: 1.9111\n",
      "[0/25][698/782] Loss_D: 0.6140 Loss_G: 3.9024\n",
      "[0/25][699/782] Loss_D: 0.4353 Loss_G: 3.3679\n",
      "[0/25][700/782] Loss_D: 0.5134 Loss_G: 4.5926\n",
      "[0/25][701/782] Loss_D: 0.4303 Loss_G: 3.3972\n",
      "[0/25][702/782] Loss_D: 0.4457 Loss_G: 4.0674\n",
      "[0/25][703/782] Loss_D: 0.3964 Loss_G: 2.8053\n",
      "[0/25][704/782] Loss_D: 0.4013 Loss_G: 4.5950\n",
      "[0/25][705/782] Loss_D: 0.2060 Loss_G: 4.2297\n",
      "[0/25][706/782] Loss_D: 0.3340 Loss_G: 3.7248\n",
      "[0/25][707/782] Loss_D: 0.1891 Loss_G: 4.7686\n",
      "[0/25][708/782] Loss_D: 0.3221 Loss_G: 2.8508\n",
      "[0/25][709/782] Loss_D: 0.4584 Loss_G: 5.5995\n",
      "[0/25][710/782] Loss_D: 0.1833 Loss_G: 4.9853\n",
      "[0/25][711/782] Loss_D: 0.1087 Loss_G: 4.0901\n",
      "[0/25][712/782] Loss_D: 0.3346 Loss_G: 3.8306\n",
      "[0/25][713/782] Loss_D: 0.1860 Loss_G: 4.7998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][714/782] Loss_D: 0.2651 Loss_G: 3.9858\n",
      "[0/25][715/782] Loss_D: 0.6669 Loss_G: 0.9295\n",
      "[0/25][716/782] Loss_D: 2.2209 Loss_G: 9.6428\n",
      "[0/25][717/782] Loss_D: 2.1051 Loss_G: 0.3214\n",
      "[0/25][718/782] Loss_D: 2.8269 Loss_G: 7.0840\n",
      "[0/25][719/782] Loss_D: 2.0729 Loss_G: 4.2516\n",
      "[0/25][720/782] Loss_D: 1.4088 Loss_G: 0.9892\n",
      "[0/25][721/782] Loss_D: 2.1664 Loss_G: 3.4116\n",
      "[0/25][722/782] Loss_D: 1.1470 Loss_G: 2.7163\n",
      "[0/25][723/782] Loss_D: 1.2076 Loss_G: 1.3325\n",
      "[0/25][724/782] Loss_D: 1.4531 Loss_G: 3.1407\n",
      "[0/25][725/782] Loss_D: 0.8277 Loss_G: 2.6969\n",
      "[0/25][726/782] Loss_D: 1.0029 Loss_G: 2.1100\n",
      "[0/25][727/782] Loss_D: 1.0526 Loss_G: 2.3997\n",
      "[0/25][728/782] Loss_D: 0.8083 Loss_G: 3.2213\n",
      "[0/25][729/782] Loss_D: 0.9902 Loss_G: 2.6744\n",
      "[0/25][730/782] Loss_D: 0.8752 Loss_G: 2.9008\n",
      "[0/25][731/782] Loss_D: 0.9428 Loss_G: 2.3822\n",
      "[0/25][732/782] Loss_D: 0.9988 Loss_G: 4.2708\n",
      "[0/25][733/782] Loss_D: 0.5085 Loss_G: 4.1366\n",
      "[0/25][734/782] Loss_D: 0.8216 Loss_G: 3.5927\n",
      "[0/25][735/782] Loss_D: 0.6757 Loss_G: 2.2769\n",
      "[0/25][736/782] Loss_D: 1.1813 Loss_G: 6.3363\n",
      "[0/25][737/782] Loss_D: 1.2478 Loss_G: 3.0814\n",
      "[0/25][738/782] Loss_D: 0.6611 Loss_G: 2.5718\n",
      "[0/25][739/782] Loss_D: 0.8292 Loss_G: 5.3615\n",
      "[0/25][740/782] Loss_D: 0.8093 Loss_G: 2.8706\n",
      "[0/25][741/782] Loss_D: 0.4605 Loss_G: 3.0615\n",
      "[0/25][742/782] Loss_D: 0.5104 Loss_G: 4.8671\n",
      "[0/25][743/782] Loss_D: 0.9466 Loss_G: 1.9020\n",
      "[0/25][744/782] Loss_D: 0.8427 Loss_G: 4.9664\n",
      "[0/25][745/782] Loss_D: 0.3547 Loss_G: 4.1994\n",
      "[0/25][746/782] Loss_D: 0.4651 Loss_G: 2.3983\n",
      "[0/25][747/782] Loss_D: 0.6062 Loss_G: 4.8127\n",
      "[0/25][748/782] Loss_D: 0.3209 Loss_G: 3.9415\n",
      "[0/25][749/782] Loss_D: 0.3294 Loss_G: 3.3011\n",
      "[0/25][750/782] Loss_D: 0.4204 Loss_G: 4.5209\n",
      "[0/25][751/782] Loss_D: 0.6094 Loss_G: 1.9824\n",
      "[0/25][752/782] Loss_D: 1.3130 Loss_G: 7.8808\n",
      "[0/25][753/782] Loss_D: 2.5885 Loss_G: 2.1097\n",
      "[0/25][754/782] Loss_D: 0.6013 Loss_G: 1.7053\n",
      "[0/25][755/782] Loss_D: 0.9110 Loss_G: 5.0251\n",
      "[0/25][756/782] Loss_D: 0.6750 Loss_G: 3.6702\n",
      "[0/25][757/782] Loss_D: 0.3525 Loss_G: 2.3252\n",
      "[0/25][758/782] Loss_D: 0.4553 Loss_G: 3.6165\n",
      "[0/25][759/782] Loss_D: 0.2998 Loss_G: 3.8398\n",
      "[0/25][760/782] Loss_D: 0.5612 Loss_G: 2.3251\n",
      "[0/25][761/782] Loss_D: 0.4777 Loss_G: 3.2261\n",
      "[0/25][762/782] Loss_D: 0.3753 Loss_G: 3.6466\n",
      "[0/25][763/782] Loss_D: 0.4167 Loss_G: 3.5675\n",
      "[0/25][764/782] Loss_D: 0.5595 Loss_G: 2.6823\n",
      "[0/25][765/782] Loss_D: 0.5696 Loss_G: 4.6104\n",
      "[0/25][766/782] Loss_D: 0.5304 Loss_G: 3.0914\n",
      "[0/25][767/782] Loss_D: 0.3916 Loss_G: 4.3875\n",
      "[0/25][768/782] Loss_D: 0.3191 Loss_G: 3.5273\n",
      "[0/25][769/782] Loss_D: 0.3624 Loss_G: 3.8865\n",
      "[0/25][770/782] Loss_D: 0.3315 Loss_G: 3.4462\n",
      "[0/25][771/782] Loss_D: 0.2319 Loss_G: 3.7687\n",
      "[0/25][772/782] Loss_D: 0.3861 Loss_G: 4.5109\n",
      "[0/25][773/782] Loss_D: 0.2418 Loss_G: 4.2107\n",
      "[0/25][774/782] Loss_D: 0.2600 Loss_G: 3.1022\n",
      "[0/25][775/782] Loss_D: 0.3038 Loss_G: 4.4057\n",
      "[0/25][776/782] Loss_D: 0.1572 Loss_G: 5.0513\n",
      "[0/25][777/782] Loss_D: 0.3922 Loss_G: 3.2950\n",
      "[0/25][778/782] Loss_D: 0.3961 Loss_G: 6.8825\n",
      "[0/25][779/782] Loss_D: 0.6014 Loss_G: 3.1928\n",
      "[0/25][780/782] Loss_D: 0.3125 Loss_G: 4.0095\n",
      "[0/25][781/782] Loss_D: 0.2835 Loss_G: 5.6480\n",
      "[1/25][0/782] Loss_D: 0.2109 Loss_G: 4.4670\n",
      "[1/25][1/782] Loss_D: 0.2696 Loss_G: 5.8078\n",
      "[1/25][2/782] Loss_D: 0.3033 Loss_G: 4.0542\n",
      "[1/25][3/782] Loss_D: 0.1750 Loss_G: 3.4467\n",
      "[1/25][4/782] Loss_D: 0.5441 Loss_G: 6.7747\n",
      "[1/25][5/782] Loss_D: 0.3151 Loss_G: 5.7515\n",
      "[1/25][6/782] Loss_D: 0.2450 Loss_G: 3.3328\n",
      "[1/25][7/782] Loss_D: 0.6601 Loss_G: 7.5821\n",
      "[1/25][8/782] Loss_D: 0.3347 Loss_G: 6.7009\n",
      "[1/25][9/782] Loss_D: 0.2090 Loss_G: 3.8014\n",
      "[1/25][10/782] Loss_D: 0.3458 Loss_G: 5.9365\n",
      "[1/25][11/782] Loss_D: 0.0755 Loss_G: 6.1508\n",
      "[1/25][12/782] Loss_D: 0.1160 Loss_G: 5.2732\n",
      "[1/25][13/782] Loss_D: 0.2035 Loss_G: 4.5158\n",
      "[1/25][14/782] Loss_D: 0.5401 Loss_G: 10.0044\n",
      "[1/25][15/782] Loss_D: 1.4022 Loss_G: 2.3002\n",
      "[1/25][16/782] Loss_D: 1.0875 Loss_G: 12.1090\n",
      "[1/25][17/782] Loss_D: 0.5812 Loss_G: 8.7995\n",
      "[1/25][18/782] Loss_D: 0.1954 Loss_G: 4.6516\n",
      "[1/25][19/782] Loss_D: 0.5036 Loss_G: 7.3262\n",
      "[1/25][20/782] Loss_D: 0.3244 Loss_G: 4.3889\n",
      "[1/25][21/782] Loss_D: 0.3178 Loss_G: 5.3368\n",
      "[1/25][22/782] Loss_D: 0.4048 Loss_G: 6.9730\n",
      "[1/25][23/782] Loss_D: 0.2770 Loss_G: 5.2322\n",
      "[1/25][24/782] Loss_D: 0.3596 Loss_G: 5.3900\n",
      "[1/25][25/782] Loss_D: 0.3413 Loss_G: 3.5863\n",
      "[1/25][26/782] Loss_D: 0.5287 Loss_G: 7.7942\n",
      "[1/25][27/782] Loss_D: 1.2696 Loss_G: 2.1836\n",
      "[1/25][28/782] Loss_D: 1.0447 Loss_G: 8.0183\n",
      "[1/25][29/782] Loss_D: 0.7498 Loss_G: 5.4616\n",
      "[1/25][30/782] Loss_D: 0.1882 Loss_G: 3.5475\n",
      "[1/25][31/782] Loss_D: 0.3642 Loss_G: 5.2009\n",
      "[1/25][32/782] Loss_D: 0.3958 Loss_G: 3.9780\n",
      "[1/25][33/782] Loss_D: 0.4168 Loss_G: 4.4549\n",
      "[1/25][34/782] Loss_D: 0.4540 Loss_G: 4.1980\n",
      "[1/25][35/782] Loss_D: 0.3185 Loss_G: 4.6203\n",
      "[1/25][36/782] Loss_D: 0.5536 Loss_G: 2.0222\n",
      "[1/25][37/782] Loss_D: 0.7889 Loss_G: 11.1931\n",
      "[1/25][38/782] Loss_D: 3.3513 Loss_G: 2.6073\n",
      "[1/25][39/782] Loss_D: 0.5797 Loss_G: 5.3172\n",
      "[1/25][40/782] Loss_D: 0.2259 Loss_G: 5.9029\n",
      "[1/25][41/782] Loss_D: 0.2500 Loss_G: 4.0978\n",
      "[1/25][42/782] Loss_D: 0.3352 Loss_G: 3.5452\n",
      "[1/25][43/782] Loss_D: 0.7725 Loss_G: 8.4766\n",
      "[1/25][44/782] Loss_D: 2.4344 Loss_G: 1.0029\n",
      "[1/25][45/782] Loss_D: 2.2407 Loss_G: 10.3483\n",
      "[1/25][46/782] Loss_D: 1.3853 Loss_G: 5.0043\n",
      "[1/25][47/782] Loss_D: 0.2255 Loss_G: 2.9175\n",
      "[1/25][48/782] Loss_D: 0.3950 Loss_G: 4.8729\n",
      "[1/25][49/782] Loss_D: 0.3566 Loss_G: 3.7766\n",
      "[1/25][50/782] Loss_D: 0.3518 Loss_G: 3.6043\n",
      "[1/25][51/782] Loss_D: 0.5094 Loss_G: 5.0978\n",
      "[1/25][52/782] Loss_D: 0.9570 Loss_G: 1.2656\n",
      "[1/25][53/782] Loss_D: 1.2409 Loss_G: 7.9000\n",
      "[1/25][54/782] Loss_D: 0.6585 Loss_G: 4.9123\n",
      "[1/25][55/782] Loss_D: 0.5751 Loss_G: 1.8711\n",
      "[1/25][56/782] Loss_D: 1.5610 Loss_G: 8.1171\n",
      "[1/25][57/782] Loss_D: 1.9574 Loss_G: 3.1582\n",
      "[1/25][58/782] Loss_D: 0.7651 Loss_G: 3.8737\n",
      "[1/25][59/782] Loss_D: 0.4093 Loss_G: 5.3015\n",
      "[1/25][60/782] Loss_D: 0.4147 Loss_G: 4.4473\n",
      "[1/25][61/782] Loss_D: 0.4773 Loss_G: 4.0032\n",
      "[1/25][62/782] Loss_D: 0.6042 Loss_G: 3.2052\n",
      "[1/25][63/782] Loss_D: 0.6557 Loss_G: 4.0995\n",
      "[1/25][64/782] Loss_D: 0.3334 Loss_G: 5.1322\n",
      "[1/25][65/782] Loss_D: 0.4330 Loss_G: 3.8559\n",
      "[1/25][66/782] Loss_D: 0.2312 Loss_G: 4.7477\n",
      "[1/25][67/782] Loss_D: 0.2535 Loss_G: 5.4274\n",
      "[1/25][68/782] Loss_D: 0.1581 Loss_G: 5.6619\n",
      "[1/25][69/782] Loss_D: 0.4270 Loss_G: 6.4399\n",
      "[1/25][70/782] Loss_D: 0.6752 Loss_G: 4.0932\n",
      "[1/25][71/782] Loss_D: 0.7397 Loss_G: 4.8072\n",
      "[1/25][72/782] Loss_D: 0.3817 Loss_G: 6.0747\n",
      "[1/25][73/782] Loss_D: 0.3989 Loss_G: 5.3909\n",
      "[1/25][74/782] Loss_D: 0.3912 Loss_G: 4.4596\n",
      "[1/25][75/782] Loss_D: 0.4386 Loss_G: 6.2863\n",
      "[1/25][76/782] Loss_D: 0.3758 Loss_G: 5.5325\n",
      "[1/25][77/782] Loss_D: 0.2816 Loss_G: 5.8476\n",
      "[1/25][78/782] Loss_D: 0.2708 Loss_G: 6.1661\n",
      "[1/25][79/782] Loss_D: 0.2907 Loss_G: 5.6094\n",
      "[1/25][80/782] Loss_D: 0.2751 Loss_G: 6.4348\n",
      "[1/25][81/782] Loss_D: 0.2639 Loss_G: 5.3364\n",
      "[1/25][82/782] Loss_D: 0.3946 Loss_G: 8.3842\n",
      "[1/25][83/782] Loss_D: 0.3825 Loss_G: 6.9147\n",
      "[1/25][84/782] Loss_D: 0.1030 Loss_G: 5.1773\n",
      "[1/25][85/782] Loss_D: 0.3982 Loss_G: 7.0378\n",
      "[1/25][86/782] Loss_D: 0.4108 Loss_G: 6.0206\n",
      "[1/25][87/782] Loss_D: 0.1650 Loss_G: 6.2995\n",
      "[1/25][88/782] Loss_D: 0.3839 Loss_G: 4.5828\n",
      "[1/25][89/782] Loss_D: 0.5487 Loss_G: 7.4933\n",
      "[1/25][90/782] Loss_D: 0.6005 Loss_G: 3.3077\n",
      "[1/25][91/782] Loss_D: 0.6082 Loss_G: 8.4813\n",
      "[1/25][92/782] Loss_D: 0.6661 Loss_G: 4.1610\n",
      "[1/25][93/782] Loss_D: 0.5432 Loss_G: 6.6503\n",
      "[1/25][94/782] Loss_D: 0.3244 Loss_G: 4.5142\n",
      "[1/25][95/782] Loss_D: 0.4002 Loss_G: 5.9188\n",
      "[1/25][96/782] Loss_D: 0.2042 Loss_G: 5.4974\n",
      "[1/25][97/782] Loss_D: 0.3078 Loss_G: 5.6363\n",
      "[1/25][98/782] Loss_D: 0.4373 Loss_G: 2.7584\n",
      "[1/25][99/782] Loss_D: 0.7999 Loss_G: 11.6529\n",
      "[1/25][100/782] Loss_D: 2.5811 Loss_G: 4.3851\n",
      "[1/25][101/782] Loss_D: 0.2595 Loss_G: 4.3520\n",
      "[1/25][102/782] Loss_D: 0.2597 Loss_G: 5.0441\n",
      "[1/25][103/782] Loss_D: 0.3842 Loss_G: 6.6462\n",
      "[1/25][104/782] Loss_D: 0.4473 Loss_G: 3.1272\n",
      "[1/25][105/782] Loss_D: 1.0756 Loss_G: 9.2113\n",
      "[1/25][106/782] Loss_D: 1.8389 Loss_G: 0.8653\n",
      "[1/25][107/782] Loss_D: 2.1292 Loss_G: 8.6082\n",
      "[1/25][108/782] Loss_D: 1.8757 Loss_G: 4.3780\n",
      "[1/25][109/782] Loss_D: 0.4924 Loss_G: 2.6780\n",
      "[1/25][110/782] Loss_D: 1.3556 Loss_G: 7.8542\n",
      "[1/25][111/782] Loss_D: 1.5131 Loss_G: 3.6299\n",
      "[1/25][112/782] Loss_D: 0.5650 Loss_G: 3.6081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][113/782] Loss_D: 0.6362 Loss_G: 4.9584\n",
      "[1/25][114/782] Loss_D: 0.4849 Loss_G: 4.1255\n",
      "[1/25][115/782] Loss_D: 0.5001 Loss_G: 3.8911\n",
      "[1/25][116/782] Loss_D: 0.2234 Loss_G: 5.2209\n",
      "[1/25][117/782] Loss_D: 0.2188 Loss_G: 4.2716\n",
      "[1/25][118/782] Loss_D: 0.1849 Loss_G: 3.7302\n",
      "[1/25][119/782] Loss_D: 0.2057 Loss_G: 4.5658\n",
      "[1/25][120/782] Loss_D: 0.0960 Loss_G: 5.0612\n",
      "[1/25][121/782] Loss_D: 0.1108 Loss_G: 4.5526\n",
      "[1/25][122/782] Loss_D: 0.2873 Loss_G: 3.4266\n",
      "[1/25][123/782] Loss_D: 0.1632 Loss_G: 4.7623\n",
      "[1/25][124/782] Loss_D: 0.1067 Loss_G: 4.9673\n",
      "[1/25][125/782] Loss_D: 0.1982 Loss_G: 3.9500\n",
      "[1/25][126/782] Loss_D: 0.1720 Loss_G: 4.6391\n",
      "[1/25][127/782] Loss_D: 0.1374 Loss_G: 4.9147\n",
      "[1/25][128/782] Loss_D: 0.2697 Loss_G: 3.7300\n",
      "[1/25][129/782] Loss_D: 0.2159 Loss_G: 4.5003\n",
      "[1/25][130/782] Loss_D: 0.1419 Loss_G: 4.6349\n",
      "[1/25][131/782] Loss_D: 0.1460 Loss_G: 4.3440\n",
      "[1/25][132/782] Loss_D: 0.1851 Loss_G: 4.5226\n",
      "[1/25][133/782] Loss_D: 0.1903 Loss_G: 4.7067\n",
      "[1/25][134/782] Loss_D: 0.1201 Loss_G: 4.9302\n",
      "[1/25][135/782] Loss_D: 0.2031 Loss_G: 3.8435\n",
      "[1/25][136/782] Loss_D: 0.3876 Loss_G: 6.9222\n",
      "[1/25][137/782] Loss_D: 0.4359 Loss_G: 4.9642\n",
      "[1/25][138/782] Loss_D: 0.2007 Loss_G: 3.9104\n",
      "[1/25][139/782] Loss_D: 0.4962 Loss_G: 7.0720\n",
      "[1/25][140/782] Loss_D: 0.1169 Loss_G: 7.3301\n",
      "[1/25][141/782] Loss_D: 0.2200 Loss_G: 5.1888\n",
      "[1/25][142/782] Loss_D: 0.2230 Loss_G: 4.5489\n",
      "[1/25][143/782] Loss_D: 0.3363 Loss_G: 5.4781\n",
      "[1/25][144/782] Loss_D: 0.2087 Loss_G: 5.3532\n",
      "[1/25][145/782] Loss_D: 0.1965 Loss_G: 4.7500\n",
      "[1/25][146/782] Loss_D: 0.5616 Loss_G: 8.3916\n",
      "[1/25][147/782] Loss_D: 0.6581 Loss_G: 5.0087\n",
      "[1/25][148/782] Loss_D: 0.5771 Loss_G: 6.7178\n",
      "[1/25][149/782] Loss_D: 0.4270 Loss_G: 5.9680\n",
      "[1/25][150/782] Loss_D: 0.3084 Loss_G: 7.9328\n",
      "[1/25][151/782] Loss_D: 0.4436 Loss_G: 6.2420\n",
      "[1/25][152/782] Loss_D: 0.2955 Loss_G: 6.0706\n",
      "[1/25][153/782] Loss_D: 0.2098 Loss_G: 8.0832\n",
      "[1/25][154/782] Loss_D: 0.1531 Loss_G: 7.2031\n",
      "[1/25][155/782] Loss_D: 0.1477 Loss_G: 6.1968\n",
      "[1/25][156/782] Loss_D: 0.1419 Loss_G: 5.8103\n",
      "[1/25][157/782] Loss_D: 0.1787 Loss_G: 6.5797\n",
      "[1/25][158/782] Loss_D: 0.1008 Loss_G: 6.4581\n",
      "[1/25][159/782] Loss_D: 0.1425 Loss_G: 6.5969\n",
      "[1/25][160/782] Loss_D: 0.1756 Loss_G: 6.4403\n",
      "[1/25][161/782] Loss_D: 0.3615 Loss_G: 5.4721\n",
      "[1/25][162/782] Loss_D: 0.3238 Loss_G: 7.4480\n",
      "[1/25][163/782] Loss_D: 0.2334 Loss_G: 5.9761\n",
      "[1/25][164/782] Loss_D: 0.3583 Loss_G: 4.8402\n",
      "[1/25][165/782] Loss_D: 0.3888 Loss_G: 6.6869\n",
      "[1/25][166/782] Loss_D: 0.1864 Loss_G: 6.1460\n",
      "[1/25][167/782] Loss_D: 0.0534 Loss_G: 6.1888\n",
      "[1/25][168/782] Loss_D: 0.0711 Loss_G: 4.9865\n",
      "[1/25][169/782] Loss_D: 0.1360 Loss_G: 5.5535\n",
      "[1/25][170/782] Loss_D: 0.0632 Loss_G: 6.0951\n",
      "[1/25][171/782] Loss_D: 0.1217 Loss_G: 5.1327\n",
      "[1/25][172/782] Loss_D: 0.1210 Loss_G: 4.8065\n",
      "[1/25][173/782] Loss_D: 0.1427 Loss_G: 5.7658\n",
      "[1/25][174/782] Loss_D: 0.3579 Loss_G: 4.6396\n",
      "[1/25][175/782] Loss_D: 0.1996 Loss_G: 6.1195\n",
      "[1/25][176/782] Loss_D: 0.1668 Loss_G: 5.3138\n",
      "[1/25][177/782] Loss_D: 0.1331 Loss_G: 5.5647\n",
      "[1/25][178/782] Loss_D: 0.0796 Loss_G: 5.6424\n",
      "[1/25][179/782] Loss_D: 0.1155 Loss_G: 4.7540\n",
      "[1/25][180/782] Loss_D: 0.1012 Loss_G: 5.0183\n",
      "[1/25][181/782] Loss_D: 0.1134 Loss_G: 5.3042\n",
      "[1/25][182/782] Loss_D: 0.1847 Loss_G: 4.5920\n",
      "[1/25][183/782] Loss_D: 0.1593 Loss_G: 4.9920\n",
      "[1/25][184/782] Loss_D: 0.1305 Loss_G: 6.2590\n",
      "[1/25][185/782] Loss_D: 0.1965 Loss_G: 4.8588\n",
      "[1/25][186/782] Loss_D: 0.3860 Loss_G: 7.9866\n",
      "[1/25][187/782] Loss_D: 0.5797 Loss_G: 4.4973\n",
      "[1/25][188/782] Loss_D: 0.2659 Loss_G: 8.1638\n",
      "[1/25][189/782] Loss_D: 0.0637 Loss_G: 8.3604\n",
      "[1/25][190/782] Loss_D: 0.1145 Loss_G: 7.0884\n",
      "[1/25][191/782] Loss_D: 0.1128 Loss_G: 5.8853\n",
      "[1/25][192/782] Loss_D: 0.0992 Loss_G: 6.6119\n",
      "[1/25][193/782] Loss_D: 0.0892 Loss_G: 5.9688\n",
      "[1/25][194/782] Loss_D: 0.1130 Loss_G: 5.3875\n",
      "[1/25][195/782] Loss_D: 0.0924 Loss_G: 6.6683\n",
      "[1/25][196/782] Loss_D: 0.0597 Loss_G: 6.1548\n",
      "[1/25][197/782] Loss_D: 0.0623 Loss_G: 5.2345\n",
      "[1/25][198/782] Loss_D: 0.0738 Loss_G: 5.6692\n",
      "[1/25][199/782] Loss_D: 0.0983 Loss_G: 5.5109\n",
      "[1/25][200/782] Loss_D: 0.0600 Loss_G: 6.4953\n",
      "[1/25][201/782] Loss_D: 0.0917 Loss_G: 5.3740\n",
      "[1/25][202/782] Loss_D: 0.1444 Loss_G: 7.6603\n",
      "[1/25][203/782] Loss_D: 0.1510 Loss_G: 5.3757\n",
      "[1/25][204/782] Loss_D: 0.1999 Loss_G: 9.3407\n",
      "[1/25][205/782] Loss_D: 0.1453 Loss_G: 7.6934\n",
      "[1/25][206/782] Loss_D: 0.0932 Loss_G: 5.8427\n",
      "[1/25][207/782] Loss_D: 0.7029 Loss_G: 14.2502\n",
      "[1/25][208/782] Loss_D: 2.6146 Loss_G: 9.5647\n",
      "[1/25][209/782] Loss_D: 0.2087 Loss_G: 6.0801\n",
      "[1/25][210/782] Loss_D: 0.5821 Loss_G: 10.6207\n",
      "[1/25][211/782] Loss_D: 0.4344 Loss_G: 6.2249\n",
      "[1/25][212/782] Loss_D: 0.1893 Loss_G: 6.1043\n",
      "[1/25][213/782] Loss_D: 0.1117 Loss_G: 5.1285\n",
      "[1/25][214/782] Loss_D: 0.0872 Loss_G: 5.3780\n",
      "[1/25][215/782] Loss_D: 0.1257 Loss_G: 5.5096\n",
      "[1/25][216/782] Loss_D: 0.1347 Loss_G: 5.3047\n",
      "[1/25][217/782] Loss_D: 0.1183 Loss_G: 4.8808\n",
      "[1/25][218/782] Loss_D: 0.1605 Loss_G: 5.4446\n",
      "[1/25][219/782] Loss_D: 0.0589 Loss_G: 5.7522\n",
      "[1/25][220/782] Loss_D: 0.2141 Loss_G: 5.2878\n",
      "[1/25][221/782] Loss_D: 0.1271 Loss_G: 5.2515\n",
      "[1/25][222/782] Loss_D: 0.1589 Loss_G: 5.0228\n",
      "[1/25][223/782] Loss_D: 0.2074 Loss_G: 6.6239\n",
      "[1/25][224/782] Loss_D: 0.0488 Loss_G: 6.6947\n",
      "[1/25][225/782] Loss_D: 0.4588 Loss_G: 11.9386\n",
      "[1/25][226/782] Loss_D: 1.7239 Loss_G: 2.5947\n",
      "[1/25][227/782] Loss_D: 1.2808 Loss_G: 12.0301\n",
      "[1/25][228/782] Loss_D: 0.2720 Loss_G: 13.5605\n",
      "[1/25][229/782] Loss_D: 0.5177 Loss_G: 8.7451\n",
      "[1/25][230/782] Loss_D: 0.0385 Loss_G: 5.9327\n",
      "[1/25][231/782] Loss_D: 0.2426 Loss_G: 6.6773\n",
      "[1/25][232/782] Loss_D: 0.0221 Loss_G: 7.0183\n",
      "[1/25][233/782] Loss_D: 0.0854 Loss_G: 5.9497\n",
      "[1/25][234/782] Loss_D: 0.0778 Loss_G: 5.9836\n",
      "[1/25][235/782] Loss_D: 0.0755 Loss_G: 5.6489\n",
      "[1/25][236/782] Loss_D: 0.0561 Loss_G: 5.5165\n",
      "[1/25][237/782] Loss_D: 0.1291 Loss_G: 6.3883\n",
      "[1/25][238/782] Loss_D: 0.1101 Loss_G: 5.5787\n",
      "[1/25][239/782] Loss_D: 0.1685 Loss_G: 5.2493\n",
      "[1/25][240/782] Loss_D: 0.2907 Loss_G: 8.4632\n",
      "[1/25][241/782] Loss_D: 0.2694 Loss_G: 5.8802\n",
      "[1/25][242/782] Loss_D: 0.6475 Loss_G: 10.6722\n",
      "[1/25][243/782] Loss_D: 0.3425 Loss_G: 8.6416\n",
      "[1/25][244/782] Loss_D: 0.2058 Loss_G: 5.0816\n",
      "[1/25][245/782] Loss_D: 0.7616 Loss_G: 13.4733\n",
      "[1/25][246/782] Loss_D: 0.6340 Loss_G: 9.8241\n",
      "[1/25][247/782] Loss_D: 0.1450 Loss_G: 6.3801\n",
      "[1/25][248/782] Loss_D: 0.6967 Loss_G: 8.9186\n",
      "[1/25][249/782] Loss_D: 0.3240 Loss_G: 5.9207\n",
      "[1/25][250/782] Loss_D: 0.6788 Loss_G: 14.3943\n",
      "[1/25][251/782] Loss_D: 0.5829 Loss_G: 10.9808\n",
      "[1/25][252/782] Loss_D: 0.2727 Loss_G: 3.5285\n",
      "[1/25][253/782] Loss_D: 1.7821 Loss_G: 16.2899\n",
      "[1/25][254/782] Loss_D: 2.9680 Loss_G: 9.1119\n",
      "[1/25][255/782] Loss_D: 0.2748 Loss_G: 2.3671\n",
      "[1/25][256/782] Loss_D: 0.6495 Loss_G: 7.0556\n",
      "[1/25][257/782] Loss_D: 0.1347 Loss_G: 8.2432\n",
      "[1/25][258/782] Loss_D: 0.1550 Loss_G: 5.2920\n",
      "[1/25][259/782] Loss_D: 0.2488 Loss_G: 4.1756\n",
      "[1/25][260/782] Loss_D: 0.6417 Loss_G: 7.2402\n",
      "[1/25][261/782] Loss_D: 1.5837 Loss_G: 2.9635\n",
      "[1/25][262/782] Loss_D: 1.5541 Loss_G: 7.7399\n",
      "[1/25][263/782] Loss_D: 0.9976 Loss_G: 5.1428\n",
      "[1/25][264/782] Loss_D: 0.8890 Loss_G: 5.3943\n",
      "[1/25][265/782] Loss_D: 1.2421 Loss_G: 5.1631\n",
      "[1/25][266/782] Loss_D: 0.7862 Loss_G: 4.0228\n",
      "[1/25][267/782] Loss_D: 1.4422 Loss_G: 7.8751\n",
      "[1/25][268/782] Loss_D: 1.2687 Loss_G: 4.1814\n",
      "[1/25][269/782] Loss_D: 0.6023 Loss_G: 3.5370\n",
      "[1/25][270/782] Loss_D: 1.2413 Loss_G: 7.5181\n",
      "[1/25][271/782] Loss_D: 1.1193 Loss_G: 5.5414\n",
      "[1/25][272/782] Loss_D: 0.3502 Loss_G: 3.3895\n",
      "[1/25][273/782] Loss_D: 0.7637 Loss_G: 8.1428\n",
      "[1/25][274/782] Loss_D: 0.8041 Loss_G: 5.2189\n",
      "[1/25][275/782] Loss_D: 0.1613 Loss_G: 3.3895\n",
      "[1/25][276/782] Loss_D: 0.4321 Loss_G: 6.4201\n",
      "[1/25][277/782] Loss_D: 0.1700 Loss_G: 6.1614\n",
      "[1/25][278/782] Loss_D: 0.4655 Loss_G: 3.4213\n",
      "[1/25][279/782] Loss_D: 0.8258 Loss_G: 8.5487\n",
      "[1/25][280/782] Loss_D: 1.6029 Loss_G: 3.5396\n",
      "[1/25][281/782] Loss_D: 0.9471 Loss_G: 6.9625\n",
      "[1/25][282/782] Loss_D: 0.5006 Loss_G: 6.3503\n",
      "[1/25][283/782] Loss_D: 0.0885 Loss_G: 5.1998\n",
      "[1/25][284/782] Loss_D: 0.1848 Loss_G: 4.7758\n",
      "[1/25][285/782] Loss_D: 0.2997 Loss_G: 5.6736\n",
      "[1/25][286/782] Loss_D: 0.2415 Loss_G: 4.9816\n",
      "[1/25][287/782] Loss_D: 0.2955 Loss_G: 4.3223\n",
      "[1/25][288/782] Loss_D: 0.4509 Loss_G: 5.3073\n",
      "[1/25][289/782] Loss_D: 0.5560 Loss_G: 5.8306\n",
      "[1/25][290/782] Loss_D: 0.5496 Loss_G: 4.5902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][291/782] Loss_D: 0.5231 Loss_G: 6.6801\n",
      "[1/25][292/782] Loss_D: 0.2110 Loss_G: 5.9473\n",
      "[1/25][293/782] Loss_D: 0.5692 Loss_G: 2.5200\n",
      "[1/25][294/782] Loss_D: 1.5799 Loss_G: 13.5685\n",
      "[1/25][295/782] Loss_D: 2.4969 Loss_G: 7.1442\n",
      "[1/25][296/782] Loss_D: 0.4464 Loss_G: 3.5723\n",
      "[1/25][297/782] Loss_D: 1.0490 Loss_G: 9.7647\n",
      "[1/25][298/782] Loss_D: 0.4735 Loss_G: 7.1376\n",
      "[1/25][299/782] Loss_D: 0.6820 Loss_G: 2.4402\n",
      "[1/25][300/782] Loss_D: 1.4689 Loss_G: 9.8434\n",
      "[1/25][301/782] Loss_D: 0.7539 Loss_G: 7.6536\n",
      "[1/25][302/782] Loss_D: 0.3938 Loss_G: 4.1587\n",
      "[1/25][303/782] Loss_D: 0.5105 Loss_G: 5.9839\n",
      "[1/25][304/782] Loss_D: 0.3455 Loss_G: 4.9807\n",
      "[1/25][305/782] Loss_D: 0.2846 Loss_G: 4.3006\n",
      "[1/25][306/782] Loss_D: 0.3792 Loss_G: 3.6340\n",
      "[1/25][307/782] Loss_D: 0.3785 Loss_G: 4.5351\n",
      "[1/25][308/782] Loss_D: 0.2751 Loss_G: 5.5276\n",
      "[1/25][309/782] Loss_D: 0.2258 Loss_G: 4.6180\n",
      "[1/25][310/782] Loss_D: 0.3669 Loss_G: 4.5688\n",
      "[1/25][311/782] Loss_D: 0.6616 Loss_G: 3.5948\n",
      "[1/25][312/782] Loss_D: 0.8382 Loss_G: 7.7154\n",
      "[1/25][313/782] Loss_D: 1.4152 Loss_G: 2.3240\n",
      "[1/25][314/782] Loss_D: 1.1901 Loss_G: 4.4765\n",
      "[1/25][315/782] Loss_D: 0.7918 Loss_G: 5.2785\n",
      "[1/25][316/782] Loss_D: 1.1122 Loss_G: 3.4458\n",
      "[1/25][317/782] Loss_D: 0.8142 Loss_G: 8.3309\n",
      "[1/25][318/782] Loss_D: 1.0677 Loss_G: 2.5639\n",
      "[1/25][319/782] Loss_D: 0.8829 Loss_G: 8.6340\n",
      "[1/25][320/782] Loss_D: 0.8887 Loss_G: 3.7599\n",
      "[1/25][321/782] Loss_D: 0.4584 Loss_G: 4.8776\n",
      "[1/25][322/782] Loss_D: 0.5660 Loss_G: 6.0915\n",
      "[1/25][323/782] Loss_D: 0.9119 Loss_G: 2.8581\n",
      "[1/25][324/782] Loss_D: 0.5265 Loss_G: 5.0333\n",
      "[1/25][325/782] Loss_D: 0.2949 Loss_G: 6.2743\n",
      "[1/25][326/782] Loss_D: 0.5352 Loss_G: 4.2713\n",
      "[1/25][327/782] Loss_D: 0.3219 Loss_G: 3.6873\n",
      "[1/25][328/782] Loss_D: 0.3330 Loss_G: 4.5377\n",
      "[1/25][329/782] Loss_D: 0.1831 Loss_G: 5.9890\n",
      "[1/25][330/782] Loss_D: 0.2152 Loss_G: 4.6847\n",
      "[1/25][331/782] Loss_D: 0.3708 Loss_G: 3.8340\n",
      "[1/25][332/782] Loss_D: 0.3224 Loss_G: 4.0465\n",
      "[1/25][333/782] Loss_D: 0.4687 Loss_G: 3.6651\n",
      "[1/25][334/782] Loss_D: 0.4109 Loss_G: 6.4728\n",
      "[1/25][335/782] Loss_D: 0.6598 Loss_G: 2.8576\n",
      "[1/25][336/782] Loss_D: 0.4983 Loss_G: 5.6197\n",
      "[1/25][337/782] Loss_D: 0.3714 Loss_G: 3.5212\n",
      "[1/25][338/782] Loss_D: 0.3095 Loss_G: 6.2178\n",
      "[1/25][339/782] Loss_D: 0.4478 Loss_G: 4.1684\n",
      "[1/25][340/782] Loss_D: 0.3833 Loss_G: 3.0858\n",
      "[1/25][341/782] Loss_D: 0.5151 Loss_G: 6.3046\n",
      "[1/25][342/782] Loss_D: 0.2541 Loss_G: 5.6107\n",
      "[1/25][343/782] Loss_D: 0.1929 Loss_G: 4.4342\n",
      "[1/25][344/782] Loss_D: 0.1591 Loss_G: 4.6491\n",
      "[1/25][345/782] Loss_D: 0.2144 Loss_G: 5.2675\n",
      "[1/25][346/782] Loss_D: 0.3470 Loss_G: 4.3042\n",
      "[1/25][347/782] Loss_D: 0.2417 Loss_G: 5.2385\n",
      "[1/25][348/782] Loss_D: 0.2288 Loss_G: 4.6168\n",
      "[1/25][349/782] Loss_D: 0.2937 Loss_G: 4.8782\n",
      "[1/25][350/782] Loss_D: 0.1627 Loss_G: 4.9108\n",
      "[1/25][351/782] Loss_D: 0.1657 Loss_G: 4.6520\n",
      "[1/25][352/782] Loss_D: 0.2733 Loss_G: 4.3468\n",
      "[1/25][353/782] Loss_D: 0.4732 Loss_G: 7.8357\n",
      "[1/25][354/782] Loss_D: 0.4226 Loss_G: 5.8395\n",
      "[1/25][355/782] Loss_D: 0.3174 Loss_G: 3.9943\n",
      "[1/25][356/782] Loss_D: 0.9051 Loss_G: 10.1242\n",
      "[1/25][357/782] Loss_D: 1.6549 Loss_G: 3.4128\n",
      "[1/25][358/782] Loss_D: 1.6234 Loss_G: 9.6808\n",
      "[1/25][359/782] Loss_D: 0.9087 Loss_G: 6.5222\n",
      "[1/25][360/782] Loss_D: 0.3275 Loss_G: 3.7746\n",
      "[1/25][361/782] Loss_D: 0.7906 Loss_G: 7.2447\n",
      "[1/25][362/782] Loss_D: 0.4696 Loss_G: 5.1858\n",
      "[1/25][363/782] Loss_D: 0.4109 Loss_G: 3.3389\n",
      "[1/25][364/782] Loss_D: 0.5642 Loss_G: 6.5070\n",
      "[1/25][365/782] Loss_D: 0.5004 Loss_G: 4.3982\n",
      "[1/25][366/782] Loss_D: 0.2891 Loss_G: 3.4963\n",
      "[1/25][367/782] Loss_D: 0.2663 Loss_G: 4.6857\n",
      "[1/25][368/782] Loss_D: 0.2083 Loss_G: 4.5696\n",
      "[1/25][369/782] Loss_D: 0.3002 Loss_G: 5.0988\n",
      "[1/25][370/782] Loss_D: 0.2326 Loss_G: 4.1778\n",
      "[1/25][371/782] Loss_D: 0.3648 Loss_G: 3.4668\n",
      "[1/25][372/782] Loss_D: 0.2190 Loss_G: 4.8839\n",
      "[1/25][373/782] Loss_D: 0.1787 Loss_G: 4.3184\n",
      "[1/25][374/782] Loss_D: 0.2365 Loss_G: 4.3657\n",
      "[1/25][375/782] Loss_D: 0.2509 Loss_G: 4.3356\n",
      "[1/25][376/782] Loss_D: 0.2222 Loss_G: 4.6505\n",
      "[1/25][377/782] Loss_D: 0.4652 Loss_G: 3.8621\n",
      "[1/25][378/782] Loss_D: 0.3460 Loss_G: 5.8169\n",
      "[1/25][379/782] Loss_D: 0.5199 Loss_G: 1.8018\n",
      "[1/25][380/782] Loss_D: 0.8158 Loss_G: 9.3942\n",
      "[1/25][381/782] Loss_D: 0.9254 Loss_G: 1.1772\n",
      "[1/25][382/782] Loss_D: 0.7644 Loss_G: 10.2335\n",
      "[1/25][383/782] Loss_D: 0.4993 Loss_G: 4.8094\n",
      "[1/25][384/782] Loss_D: 0.2115 Loss_G: 3.7352\n",
      "[1/25][385/782] Loss_D: 0.8403 Loss_G: 7.2081\n",
      "[1/25][386/782] Loss_D: 1.1827 Loss_G: 1.3296\n",
      "[1/25][387/782] Loss_D: 0.9361 Loss_G: 5.0367\n",
      "[1/25][388/782] Loss_D: 0.9164 Loss_G: 2.0878\n",
      "[1/25][389/782] Loss_D: 0.7965 Loss_G: 2.9485\n",
      "[1/25][390/782] Loss_D: 0.3513 Loss_G: 4.2956\n",
      "[1/25][391/782] Loss_D: 0.4239 Loss_G: 3.1198\n",
      "[1/25][392/782] Loss_D: 0.3128 Loss_G: 4.1593\n",
      "[1/25][393/782] Loss_D: 0.4401 Loss_G: 2.9438\n",
      "[1/25][394/782] Loss_D: 0.2993 Loss_G: 2.8458\n",
      "[1/25][395/782] Loss_D: 0.3517 Loss_G: 4.4366\n",
      "[1/25][396/782] Loss_D: 0.6113 Loss_G: 2.5642\n",
      "[1/25][397/782] Loss_D: 0.4383 Loss_G: 3.6159\n",
      "[1/25][398/782] Loss_D: 0.3524 Loss_G: 5.2776\n",
      "[1/25][399/782] Loss_D: 1.0047 Loss_G: 1.4381\n",
      "[1/25][400/782] Loss_D: 1.0018 Loss_G: 6.2876\n",
      "[1/25][401/782] Loss_D: 0.5715 Loss_G: 3.6836\n",
      "[1/25][402/782] Loss_D: 0.2608 Loss_G: 2.8198\n",
      "[1/25][403/782] Loss_D: 0.4963 Loss_G: 4.3875\n",
      "[1/25][404/782] Loss_D: 0.4882 Loss_G: 3.2005\n",
      "[1/25][405/782] Loss_D: 0.5613 Loss_G: 2.7092\n",
      "[1/25][406/782] Loss_D: 0.8097 Loss_G: 3.0735\n",
      "[1/25][407/782] Loss_D: 0.7241 Loss_G: 7.5538\n",
      "[1/25][408/782] Loss_D: 1.6372 Loss_G: 2.1626\n",
      "[1/25][409/782] Loss_D: 0.8064 Loss_G: 6.6489\n",
      "[1/25][410/782] Loss_D: 0.6038 Loss_G: 3.7885\n",
      "[1/25][411/782] Loss_D: 0.2352 Loss_G: 3.6301\n",
      "[1/25][412/782] Loss_D: 0.6107 Loss_G: 5.9646\n",
      "[1/25][413/782] Loss_D: 0.6630 Loss_G: 2.9575\n",
      "[1/25][414/782] Loss_D: 0.6909 Loss_G: 7.0817\n",
      "[1/25][415/782] Loss_D: 0.4876 Loss_G: 5.0758\n",
      "[1/25][416/782] Loss_D: 0.3493 Loss_G: 2.4639\n",
      "[1/25][417/782] Loss_D: 0.6753 Loss_G: 6.5573\n",
      "[1/25][418/782] Loss_D: 0.1528 Loss_G: 6.6732\n",
      "[1/25][419/782] Loss_D: 0.2277 Loss_G: 4.2951\n",
      "[1/25][420/782] Loss_D: 0.3068 Loss_G: 4.0279\n",
      "[1/25][421/782] Loss_D: 0.3832 Loss_G: 4.4854\n",
      "[1/25][422/782] Loss_D: 0.7627 Loss_G: 4.9397\n",
      "[1/25][423/782] Loss_D: 0.6389 Loss_G: 2.6101\n",
      "[1/25][424/782] Loss_D: 0.7992 Loss_G: 4.1115\n",
      "[1/25][425/782] Loss_D: 0.6325 Loss_G: 3.8145\n",
      "[1/25][426/782] Loss_D: 0.1971 Loss_G: 4.5601\n",
      "[1/25][427/782] Loss_D: 0.2415 Loss_G: 4.7426\n",
      "[1/25][428/782] Loss_D: 0.2660 Loss_G: 3.5030\n",
      "[1/25][429/782] Loss_D: 0.3566 Loss_G: 4.8856\n",
      "[1/25][430/782] Loss_D: 0.4362 Loss_G: 1.5480\n",
      "[1/25][431/782] Loss_D: 1.0138 Loss_G: 9.5909\n",
      "[1/25][432/782] Loss_D: 2.6137 Loss_G: 2.0362\n",
      "[1/25][433/782] Loss_D: 0.4321 Loss_G: 3.4929\n",
      "[1/25][434/782] Loss_D: 0.5110 Loss_G: 3.5254\n",
      "[1/25][435/782] Loss_D: 0.4455 Loss_G: 2.8267\n",
      "[1/25][436/782] Loss_D: 0.5058 Loss_G: 3.8169\n",
      "[1/25][437/782] Loss_D: 0.6976 Loss_G: 2.4069\n",
      "[1/25][438/782] Loss_D: 0.8169 Loss_G: 6.6224\n",
      "[1/25][439/782] Loss_D: 1.5996 Loss_G: 1.1385\n",
      "[1/25][440/782] Loss_D: 0.9766 Loss_G: 5.8983\n",
      "[1/25][441/782] Loss_D: 0.1909 Loss_G: 5.9889\n",
      "[1/25][442/782] Loss_D: 0.2630 Loss_G: 4.0642\n",
      "[1/25][443/782] Loss_D: 0.4234 Loss_G: 3.6555\n",
      "[1/25][444/782] Loss_D: 0.3689 Loss_G: 3.2812\n",
      "[1/25][445/782] Loss_D: 0.5779 Loss_G: 4.5146\n",
      "[1/25][446/782] Loss_D: 0.4945 Loss_G: 3.3264\n",
      "[1/25][447/782] Loss_D: 0.3914 Loss_G: 5.1017\n",
      "[1/25][448/782] Loss_D: 0.3824 Loss_G: 3.8105\n",
      "[1/25][449/782] Loss_D: 0.3676 Loss_G: 3.9059\n",
      "[1/25][450/782] Loss_D: 0.2360 Loss_G: 4.7613\n",
      "[1/25][451/782] Loss_D: 0.2426 Loss_G: 4.4366\n",
      "[1/25][452/782] Loss_D: 0.9852 Loss_G: 3.1428\n",
      "[1/25][453/782] Loss_D: 0.6139 Loss_G: 5.3650\n",
      "[1/25][454/782] Loss_D: 0.2731 Loss_G: 4.6149\n",
      "[1/25][455/782] Loss_D: 0.4537 Loss_G: 3.0567\n",
      "[1/25][456/782] Loss_D: 0.4537 Loss_G: 5.6989\n",
      "[1/25][457/782] Loss_D: 1.0919 Loss_G: 1.2909\n",
      "[1/25][458/782] Loss_D: 1.4937 Loss_G: 7.0872\n",
      "[1/25][459/782] Loss_D: 1.5725 Loss_G: 3.5724\n",
      "[1/25][460/782] Loss_D: 0.2248 Loss_G: 2.6438\n",
      "[1/25][461/782] Loss_D: 0.5717 Loss_G: 4.6746\n",
      "[1/25][462/782] Loss_D: 0.2924 Loss_G: 4.5888\n",
      "[1/25][463/782] Loss_D: 0.4954 Loss_G: 3.1352\n",
      "[1/25][464/782] Loss_D: 0.4432 Loss_G: 4.3451\n",
      "[1/25][465/782] Loss_D: 0.4643 Loss_G: 3.3798\n",
      "[1/25][466/782] Loss_D: 0.3021 Loss_G: 3.0244\n",
      "[1/25][467/782] Loss_D: 0.7923 Loss_G: 5.4536\n",
      "[1/25][468/782] Loss_D: 0.4108 Loss_G: 4.4484\n",
      "[1/25][469/782] Loss_D: 0.1859 Loss_G: 3.3981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][470/782] Loss_D: 0.3925 Loss_G: 4.3226\n",
      "[1/25][471/782] Loss_D: 0.2030 Loss_G: 4.3816\n",
      "[1/25][472/782] Loss_D: 0.3225 Loss_G: 3.6159\n",
      "[1/25][473/782] Loss_D: 0.7518 Loss_G: 5.8751\n",
      "[1/25][474/782] Loss_D: 1.2393 Loss_G: 1.6024\n",
      "[1/25][475/782] Loss_D: 1.2841 Loss_G: 7.2688\n",
      "[1/25][476/782] Loss_D: 1.0744 Loss_G: 1.7594\n",
      "[1/25][477/782] Loss_D: 1.0543 Loss_G: 5.8872\n",
      "[1/25][478/782] Loss_D: 1.0081 Loss_G: 3.7088\n",
      "[1/25][479/782] Loss_D: 0.2290 Loss_G: 4.6937\n",
      "[1/25][480/782] Loss_D: 0.3455 Loss_G: 5.0464\n",
      "[1/25][481/782] Loss_D: 0.4812 Loss_G: 5.1480\n",
      "[1/25][482/782] Loss_D: 0.4859 Loss_G: 3.5167\n",
      "[1/25][483/782] Loss_D: 0.6227 Loss_G: 4.6635\n",
      "[1/25][484/782] Loss_D: 0.5440 Loss_G: 3.4125\n",
      "[1/25][485/782] Loss_D: 0.7649 Loss_G: 4.7941\n",
      "[1/25][486/782] Loss_D: 0.8167 Loss_G: 2.3843\n",
      "[1/25][487/782] Loss_D: 0.7773 Loss_G: 5.4751\n",
      "[1/25][488/782] Loss_D: 0.9861 Loss_G: 2.4084\n",
      "[1/25][489/782] Loss_D: 0.5278 Loss_G: 3.5336\n",
      "[1/25][490/782] Loss_D: 0.2713 Loss_G: 4.3854\n",
      "[1/25][491/782] Loss_D: 0.3176 Loss_G: 4.5393\n",
      "[1/25][492/782] Loss_D: 0.4727 Loss_G: 3.2731\n",
      "[1/25][493/782] Loss_D: 0.4248 Loss_G: 3.5809\n",
      "[1/25][494/782] Loss_D: 0.3686 Loss_G: 4.3356\n",
      "[1/25][495/782] Loss_D: 0.7126 Loss_G: 2.2213\n",
      "[1/25][496/782] Loss_D: 0.5859 Loss_G: 5.4321\n",
      "[1/25][497/782] Loss_D: 0.4910 Loss_G: 3.7961\n",
      "[1/25][498/782] Loss_D: 0.3487 Loss_G: 3.3223\n",
      "[1/25][499/782] Loss_D: 0.4910 Loss_G: 3.3576\n",
      "[1/25][500/782] Loss_D: 0.4723 Loss_G: 5.4819\n",
      "[1/25][501/782] Loss_D: 0.4916 Loss_G: 3.3611\n",
      "[1/25][502/782] Loss_D: 0.3607 Loss_G: 3.9838\n",
      "[1/25][503/782] Loss_D: 0.3784 Loss_G: 3.3017\n",
      "[1/25][504/782] Loss_D: 0.4634 Loss_G: 4.2361\n",
      "[1/25][505/782] Loss_D: 0.5764 Loss_G: 4.3436\n",
      "[1/25][506/782] Loss_D: 0.4173 Loss_G: 3.0315\n",
      "[1/25][507/782] Loss_D: 0.4557 Loss_G: 5.0498\n",
      "[1/25][508/782] Loss_D: 0.5827 Loss_G: 2.5373\n",
      "[1/25][509/782] Loss_D: 0.4956 Loss_G: 4.1282\n",
      "[1/25][510/782] Loss_D: 0.3577 Loss_G: 4.3857\n",
      "[1/25][511/782] Loss_D: 0.4832 Loss_G: 3.0590\n",
      "[1/25][512/782] Loss_D: 0.3831 Loss_G: 4.3860\n",
      "[1/25][513/782] Loss_D: 0.2878 Loss_G: 4.0022\n",
      "[1/25][514/782] Loss_D: 0.3610 Loss_G: 2.9776\n",
      "[1/25][515/782] Loss_D: 0.3442 Loss_G: 3.6256\n",
      "[1/25][516/782] Loss_D: 0.3480 Loss_G: 4.0740\n",
      "[1/25][517/782] Loss_D: 0.2337 Loss_G: 4.1745\n",
      "[1/25][518/782] Loss_D: 0.2966 Loss_G: 3.2139\n",
      "[1/25][519/782] Loss_D: 0.3571 Loss_G: 4.1383\n",
      "[1/25][520/782] Loss_D: 0.2783 Loss_G: 5.7128\n",
      "[1/25][521/782] Loss_D: 0.3471 Loss_G: 3.3608\n",
      "[1/25][522/782] Loss_D: 0.4404 Loss_G: 5.6167\n",
      "[1/25][523/782] Loss_D: 0.2440 Loss_G: 4.2254\n",
      "[1/25][524/782] Loss_D: 0.1853 Loss_G: 4.5210\n",
      "[1/25][525/782] Loss_D: 0.3173 Loss_G: 3.8585\n",
      "[1/25][526/782] Loss_D: 0.5058 Loss_G: 5.3079\n",
      "[1/25][527/782] Loss_D: 0.4527 Loss_G: 3.6183\n",
      "[1/25][528/782] Loss_D: 0.3794 Loss_G: 3.6651\n",
      "[1/25][529/782] Loss_D: 0.4177 Loss_G: 5.8771\n",
      "[1/25][530/782] Loss_D: 0.3501 Loss_G: 4.0022\n",
      "[1/25][531/782] Loss_D: 0.3914 Loss_G: 3.7230\n",
      "[1/25][532/782] Loss_D: 0.6598 Loss_G: 6.7866\n",
      "[1/25][533/782] Loss_D: 0.8068 Loss_G: 1.7223\n",
      "[1/25][534/782] Loss_D: 1.1918 Loss_G: 7.6486\n",
      "[1/25][535/782] Loss_D: 0.8001 Loss_G: 5.7142\n",
      "[1/25][536/782] Loss_D: 0.2089 Loss_G: 2.9564\n",
      "[1/25][537/782] Loss_D: 0.8168 Loss_G: 8.1099\n",
      "[1/25][538/782] Loss_D: 0.3713 Loss_G: 6.0659\n",
      "[1/25][539/782] Loss_D: 0.2411 Loss_G: 3.8447\n",
      "[1/25][540/782] Loss_D: 0.2311 Loss_G: 3.9017\n",
      "[1/25][541/782] Loss_D: 0.4340 Loss_G: 4.5336\n",
      "[1/25][542/782] Loss_D: 0.4786 Loss_G: 2.1554\n",
      "[1/25][543/782] Loss_D: 1.2764 Loss_G: 7.6786\n",
      "[1/25][544/782] Loss_D: 1.4323 Loss_G: 1.5851\n",
      "[1/25][545/782] Loss_D: 1.2737 Loss_G: 5.6428\n",
      "[1/25][546/782] Loss_D: 0.8634 Loss_G: 3.3223\n",
      "[1/25][547/782] Loss_D: 0.3630 Loss_G: 2.8778\n",
      "[1/25][548/782] Loss_D: 0.2890 Loss_G: 4.8421\n",
      "[1/25][549/782] Loss_D: 0.2666 Loss_G: 3.9322\n",
      "[1/25][550/782] Loss_D: 0.4253 Loss_G: 3.9226\n",
      "[1/25][551/782] Loss_D: 0.4671 Loss_G: 3.5601\n",
      "[1/25][552/782] Loss_D: 0.5318 Loss_G: 4.2578\n",
      "[1/25][553/782] Loss_D: 0.4235 Loss_G: 4.1088\n",
      "[1/25][554/782] Loss_D: 0.5669 Loss_G: 2.7487\n",
      "[1/25][555/782] Loss_D: 0.6684 Loss_G: 5.5034\n",
      "[1/25][556/782] Loss_D: 0.7793 Loss_G: 2.2295\n",
      "[1/25][557/782] Loss_D: 0.4121 Loss_G: 3.9813\n",
      "[1/25][558/782] Loss_D: 0.3724 Loss_G: 4.2487\n",
      "[1/25][559/782] Loss_D: 0.3625 Loss_G: 3.1767\n",
      "[1/25][560/782] Loss_D: 0.3520 Loss_G: 3.6055\n",
      "[1/25][561/782] Loss_D: 0.2792 Loss_G: 4.1845\n",
      "[1/25][562/782] Loss_D: 0.4832 Loss_G: 2.7133\n",
      "[1/25][563/782] Loss_D: 0.2486 Loss_G: 3.5866\n",
      "[1/25][564/782] Loss_D: 0.1710 Loss_G: 4.3278\n",
      "[1/25][565/782] Loss_D: 0.2002 Loss_G: 3.9833\n",
      "[1/25][566/782] Loss_D: 0.1446 Loss_G: 4.2062\n",
      "[1/25][567/782] Loss_D: 0.2823 Loss_G: 2.9575\n",
      "[1/25][568/782] Loss_D: 0.4889 Loss_G: 5.5916\n",
      "[1/25][569/782] Loss_D: 0.5758 Loss_G: 2.3659\n",
      "[1/25][570/782] Loss_D: 0.5021 Loss_G: 1.9982\n",
      "[1/25][571/782] Loss_D: 0.9554 Loss_G: 8.0851\n",
      "[1/25][572/782] Loss_D: 1.6049 Loss_G: 2.4798\n",
      "[1/25][573/782] Loss_D: 0.6190 Loss_G: 3.8661\n",
      "[1/25][574/782] Loss_D: 0.3226 Loss_G: 5.1815\n",
      "[1/25][575/782] Loss_D: 0.3061 Loss_G: 3.6204\n",
      "[1/25][576/782] Loss_D: 0.3741 Loss_G: 2.2940\n",
      "[1/25][577/782] Loss_D: 0.4708 Loss_G: 4.8014\n",
      "[1/25][578/782] Loss_D: 0.3426 Loss_G: 3.6389\n",
      "[1/25][579/782] Loss_D: 0.4437 Loss_G: 1.9652\n",
      "[1/25][580/782] Loss_D: 0.4167 Loss_G: 4.2081\n",
      "[1/25][581/782] Loss_D: 0.3087 Loss_G: 3.6723\n",
      "[1/25][582/782] Loss_D: 0.1742 Loss_G: 3.4790\n",
      "[1/25][583/782] Loss_D: 0.3218 Loss_G: 3.8611\n",
      "[1/25][584/782] Loss_D: 0.2275 Loss_G: 3.8402\n",
      "[1/25][585/782] Loss_D: 0.2474 Loss_G: 3.4579\n",
      "[1/25][586/782] Loss_D: 0.3271 Loss_G: 4.2968\n",
      "[1/25][587/782] Loss_D: 0.2404 Loss_G: 3.7642\n",
      "[1/25][588/782] Loss_D: 0.2877 Loss_G: 3.8405\n",
      "[1/25][589/782] Loss_D: 0.3302 Loss_G: 4.4286\n",
      "[1/25][590/782] Loss_D: 0.6139 Loss_G: 1.9530\n",
      "[1/25][591/782] Loss_D: 0.3961 Loss_G: 4.9309\n",
      "[1/25][592/782] Loss_D: 0.3120 Loss_G: 4.3914\n",
      "[1/25][593/782] Loss_D: 0.2594 Loss_G: 3.5407\n",
      "[1/25][594/782] Loss_D: 0.2193 Loss_G: 3.5304\n",
      "[1/25][595/782] Loss_D: 0.4064 Loss_G: 5.2471\n",
      "[1/25][596/782] Loss_D: 0.4018 Loss_G: 3.6496\n",
      "[1/25][597/782] Loss_D: 0.4652 Loss_G: 3.3422\n",
      "[1/25][598/782] Loss_D: 0.5356 Loss_G: 5.3183\n",
      "[1/25][599/782] Loss_D: 0.8150 Loss_G: 1.6948\n",
      "[1/25][600/782] Loss_D: 0.8356 Loss_G: 6.6912\n",
      "[1/25][601/782] Loss_D: 0.3840 Loss_G: 5.0989\n",
      "[1/25][602/782] Loss_D: 0.4581 Loss_G: 2.1351\n",
      "[1/25][603/782] Loss_D: 0.5540 Loss_G: 5.2979\n",
      "[1/25][604/782] Loss_D: 0.2331 Loss_G: 4.6824\n",
      "[1/25][605/782] Loss_D: 0.2343 Loss_G: 3.7446\n",
      "[1/25][606/782] Loss_D: 0.3548 Loss_G: 4.3337\n",
      "[1/25][607/782] Loss_D: 0.4362 Loss_G: 4.0197\n",
      "[1/25][608/782] Loss_D: 0.3504 Loss_G: 3.1733\n",
      "[1/25][609/782] Loss_D: 0.5152 Loss_G: 4.3837\n",
      "[1/25][610/782] Loss_D: 0.2087 Loss_G: 4.6267\n",
      "[1/25][611/782] Loss_D: 0.3634 Loss_G: 2.7583\n",
      "[1/25][612/782] Loss_D: 0.3264 Loss_G: 4.7386\n",
      "[1/25][613/782] Loss_D: 0.1642 Loss_G: 5.0779\n",
      "[1/25][614/782] Loss_D: 0.4218 Loss_G: 2.2569\n",
      "[1/25][615/782] Loss_D: 0.6454 Loss_G: 6.9938\n",
      "[1/25][616/782] Loss_D: 0.5347 Loss_G: 3.6241\n",
      "[1/25][617/782] Loss_D: 0.2656 Loss_G: 3.0244\n",
      "[1/25][618/782] Loss_D: 0.4118 Loss_G: 5.7988\n",
      "[1/25][619/782] Loss_D: 0.4754 Loss_G: 2.6958\n",
      "[1/25][620/782] Loss_D: 0.5437 Loss_G: 5.8734\n",
      "[1/25][621/782] Loss_D: 0.7761 Loss_G: 2.3278\n",
      "[1/25][622/782] Loss_D: 0.9631 Loss_G: 8.2628\n",
      "[1/25][623/782] Loss_D: 1.3768 Loss_G: 3.2550\n",
      "[1/25][624/782] Loss_D: 0.3243 Loss_G: 2.7187\n",
      "[1/25][625/782] Loss_D: 0.5084 Loss_G: 6.1942\n",
      "[1/25][626/782] Loss_D: 0.2108 Loss_G: 4.7969\n",
      "[1/25][627/782] Loss_D: 0.1649 Loss_G: 3.1273\n",
      "[1/25][628/782] Loss_D: 0.7175 Loss_G: 6.7760\n",
      "[1/25][629/782] Loss_D: 1.0802 Loss_G: 1.3292\n",
      "[1/25][630/782] Loss_D: 0.9947 Loss_G: 4.4679\n",
      "[1/25][631/782] Loss_D: 0.7378 Loss_G: 2.3101\n",
      "[1/25][632/782] Loss_D: 0.8724 Loss_G: 5.1257\n",
      "[1/25][633/782] Loss_D: 0.7916 Loss_G: 1.7288\n",
      "[1/25][634/782] Loss_D: 0.8663 Loss_G: 6.3444\n",
      "[1/25][635/782] Loss_D: 0.6927 Loss_G: 3.3019\n",
      "[1/25][636/782] Loss_D: 0.6143 Loss_G: 4.8120\n",
      "[1/25][637/782] Loss_D: 0.4329 Loss_G: 3.6790\n",
      "[1/25][638/782] Loss_D: 0.9317 Loss_G: 3.7479\n",
      "[1/25][639/782] Loss_D: 0.5466 Loss_G: 3.3077\n",
      "[1/25][640/782] Loss_D: 0.5228 Loss_G: 3.9279\n",
      "[1/25][641/782] Loss_D: 0.5911 Loss_G: 2.4294\n",
      "[1/25][642/782] Loss_D: 0.7988 Loss_G: 5.9073\n",
      "[1/25][643/782] Loss_D: 0.3102 Loss_G: 4.9231\n",
      "[1/25][644/782] Loss_D: 0.1764 Loss_G: 3.3360\n",
      "[1/25][645/782] Loss_D: 0.1695 Loss_G: 4.1426\n",
      "[1/25][646/782] Loss_D: 0.3302 Loss_G: 6.2485\n",
      "[1/25][647/782] Loss_D: 0.3403 Loss_G: 4.6671\n",
      "[1/25][648/782] Loss_D: 0.3809 Loss_G: 2.1475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/25][649/782] Loss_D: 0.7936 Loss_G: 9.2326\n",
      "[1/25][650/782] Loss_D: 1.4939 Loss_G: 2.9129\n",
      "[1/25][651/782] Loss_D: 0.6760 Loss_G: 7.2807\n",
      "[1/25][652/782] Loss_D: 0.4454 Loss_G: 3.5811\n",
      "[1/25][653/782] Loss_D: 0.6109 Loss_G: 3.4742\n",
      "[1/25][654/782] Loss_D: 0.7412 Loss_G: 6.9484\n",
      "[1/25][655/782] Loss_D: 1.2518 Loss_G: 1.8025\n",
      "[1/25][656/782] Loss_D: 1.4105 Loss_G: 7.6194\n",
      "[1/25][657/782] Loss_D: 1.3022 Loss_G: 2.7797\n",
      "[1/25][658/782] Loss_D: 0.6941 Loss_G: 5.3621\n",
      "[1/25][659/782] Loss_D: 0.4500 Loss_G: 4.8459\n",
      "[1/25][660/782] Loss_D: 0.3252 Loss_G: 3.5358\n",
      "[1/25][661/782] Loss_D: 0.3668 Loss_G: 4.0123\n",
      "[1/25][662/782] Loss_D: 0.5201 Loss_G: 3.7540\n",
      "[1/25][663/782] Loss_D: 0.3227 Loss_G: 3.5957\n",
      "[1/25][664/782] Loss_D: 0.3014 Loss_G: 3.5625\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "for epoch in range(25):\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data\n",
    "        input = Variable(real)\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
    "        fake = netG(noise)\n",
    "        target = Variable(torch.zeros(input.size()[0]))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step() \n",
    "        \n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
